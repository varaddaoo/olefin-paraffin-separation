{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f1202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "#mpl.rc('figure', max_open_warning = 0)\n",
    "#%matplotlib inline\n",
    "#%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class style:\n",
    "   BOLD = '\\033[1m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd() # Getting current directory\n",
    "descriptor_in_path = os.path.join(PATH, '../input/descriptor.csv')\n",
    "\n",
    "df_descriptor = pd.read_csv(descriptor_in_path)\n",
    "\n",
    "print(f'Descriptor input DataFrame shape:\\n\\n {df_descriptor.shape}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "print(f'\\nDescriptor input data columns:\\n\\n {df_descriptor.columns}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "print(f'\\nDescriptor input dataframe head:\\n\\n {df_descriptor.head()}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "del descriptor_in_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f2bea",
   "metadata": {},
   "source": [
    "## Renaming descriptor columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a9e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {'name': 'mof', 'Di': 'LCD', 'Df': 'PLD', 'ASA(m2/gram)_1.9': 'GSA', \n",
    "               'AV_Volume_fraction_1.9': 'AVF', 'AV(cm3/gram)_1.9': 'GPV', 'density(gram_cm3)': 'Density'}\n",
    "\n",
    "df_descriptor = df_descriptor.rename(columns=rename_dict)\n",
    "\n",
    "print(f'\\nCurated descriptor columns:\\n\\n {df_descriptor.columns}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "print(df_descriptor.dtypes) # Prints the datatype of each column in dataframe\n",
    "del rename_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5abbda",
   "metadata": {},
   "source": [
    "## Curating descriptor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3506d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_descriptor_gross1_atomic = df_descriptor\n",
    "\n",
    "# Selecting materials with PLD > 3.8 A\n",
    "\n",
    "df_descriptor_gross1_atomic = df_descriptor_gross1_atomic[(df_descriptor_gross1_atomic['PLD'] > 3.8)]\n",
    "\n",
    "# Selecting materials with non-zero void fraction\n",
    "\n",
    "df_descriptor_gross1_atomic = df_descriptor_gross1_atomic[(df_descriptor_gross1_atomic['AVF'] > 0.0)]\n",
    "\n",
    "descriptor_mof_name = df_descriptor_gross1_atomic['mof'].astype(str)\n",
    "\n",
    "PATH = os.getcwd() # Getting current directory\n",
    "curated_mof_name = os.path.join(PATH, '../output/curated-mof.csv')\n",
    "descriptor_mof_name.to_csv(curated_mof_name, index=False)\n",
    "\n",
    "columns = ['PLD', 'LCD', 'GSA', 'AVF', 'GPV', 'Density', 'total_degree_unsaturation', 'degree_unsaturation', \n",
    "           'metallic_percentage', 'O_to_Metal_ration', 'N_to_O_ratio', 'H' ,'Ni', 'Co', 'Cu', 'Zn', 'Pb', 'Mn',\n",
    "           'Cd', 'C', 'O', 'N', 'S', 'Cl', 'Br', 'F', 'I']\n",
    "\n",
    "shap_columns = columns\n",
    "\n",
    "df_descriptor_gross1_atomic = df_descriptor_gross1_atomic[columns].astype(float)\n",
    "curated_mof_prop = os.path.join(PATH, '../output/curated-mof-prop.csv')\n",
    "\n",
    "df_descriptor_gross1_atomic.to_csv(curated_mof_prop, index=False)\n",
    "\n",
    "print(f'\\nCurated gross1_atomic descriptor data:\\n\\n {df_descriptor_gross1_atomic}\\n')\n",
    "print('\\n------------------------------------------------------------\\n')\n",
    "\n",
    "print(f'\\nData type of each column. Note that it should be float\\n\\n {df_descriptor_gross1_atomic.dtypes}\\n')\n",
    "print('\\n------------------------------------------------------------\\n')\n",
    "\n",
    "del df_descriptor\n",
    "del columns\n",
    "del descriptor_mof_name\n",
    "del curated_mof_name\n",
    "del curated_mof_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae88ece",
   "metadata": {},
   "source": [
    "## Taking look at target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfb53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_in_path = os.path.join(PATH, '../input/C3H8-C3H6.csv')\n",
    "#target_in_path = os.path.join(PATH, '../input/C2H6-C2H4.csv')\n",
    "\n",
    "df_target = pd.read_csv(target_in_path)\n",
    "\n",
    "print(f'Target property input DataFrame shape:\\n\\n {df_target.shape}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "print(f'\\nTarget property input data columns:\\n\\n {df_target.columns}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "print(f'\\nTarget property input dataframe head:\\n\\n {df_target.head()}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "del target_in_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2582f91",
   "metadata": {},
   "source": [
    "## Renaming Target property columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb4d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {'MOF_no': 'mof', 'propane_avg(mol/kg)': 'propane_uptake(mol/kg)',\n",
    "              'propylene_avg(mol/kg)': 'propylene_uptake(mol/kg)',\n",
    "              'C3H8/C3H6 Selectivity (1Bar)': 'propane_propylene_selectivity', 'Df': 'PLD',\n",
    "              'AV_Volume_fraction_1.9': 'AVF'}\n",
    "'''\n",
    "\n",
    "rename_dict = {'MOF_no': 'mof', 'ethane_avg(mol/kg)': 'ethane_uptake(mol/kg)',\n",
    "              'ethylene_avg(mol/kg)': 'ethylene_uptake(mol/kg)',\n",
    "              'C2H6/C2H4 Selectivity (1Bar)': 'ethane_ethylene_selectivity', 'Df': 'PLD',\n",
    "              'AV_Volume_fraction_1.9': 'AVF'}\n",
    "\n",
    "'''\n",
    "df_target = df_target.rename(columns=rename_dict)\n",
    "\n",
    "print(f'\\nCurated target columns:\\n\\n {df_target.columns}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "      \n",
    "del rename_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cfe5ff",
   "metadata": {},
   "source": [
    "## Curating Target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_gross1_atomic = df_target\n",
    "\n",
    "# Selecting materials with PLD > 3.8 A\n",
    "\n",
    "df_target_gross1_atomic = df_target_gross1_atomic[(df_target_gross1_atomic['PLD'] > 3.8)]\n",
    "\n",
    "# Selecting material with AVF > 0\n",
    "df_target_gross1_atomic = df_target_gross1_atomic[(df_target_gross1_atomic['AVF'] > 0.0)]\n",
    "\n",
    "target_mof_name = df_target_gross1_atomic['mof'].astype(str)\n",
    "target_mof_name_path = os.path.join(PATH, '../output/target-mof-name.csv')\n",
    "target_mof_name.to_csv(target_mof_name_path, index=False)\n",
    "\n",
    "columns = ['propane_uptake(mol/kg)', 'propane_propylene_selectivity', 'TSN', 'propylene_uptake(mol/kg)']\n",
    "\n",
    "#columns = ['ethane_uptake(mol/kg)', 'ethane_ethylene_selectivity', 'TSN', 'ethylene_uptake(mol/kg)']\n",
    "\n",
    "\n",
    "df_target_gross1_atomic = df_target_gross1_atomic[columns].astype(float)\n",
    "target_mof_prop_path = os.path.join(PATH, '../output/target-mof-prop.csv')\n",
    "\n",
    "print(f'\\nCurated target data:\\n\\n {df_target_gross1_atomic}\\n')\n",
    "print('\\n------------------------------------------------------------\\n')\n",
    "\n",
    "print(f'\\nData type of each column. Note that it should be float\\n\\n {df_target_gross1_atomic.dtypes}\\n')\n",
    "print('\\n------------------------------------------------------------\\n')\n",
    "\n",
    "del df_target\n",
    "del columns\n",
    "del target_mof_name\n",
    "del target_mof_name_path\n",
    "del target_mof_prop_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "profile = ProfileReport(df_join.copy(),title='C3H8-C3H6', html={'style':{'full_width':True}})\n",
    "# profile.to_widgets()\n",
    "#profile.to_notebook_iframe()\n",
    "C3H8_report = os.path.join(PATH, '../output/C3H8-C3H6-report.csv')\n",
    "\n",
    "profile.to_file(\"/home/varad/Pictures/best_model_selection_updated/1_excluding_oms/1_Propane_RACs_excluding.html\")\n",
    "\n",
    "''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_crude = df_descriptor_gross1_atomic\n",
    "Y_crude = df_target_gross1_atomic\n",
    "\n",
    "print(f'\\nShape of X_crude: {X_crude.shape}')\n",
    "print(f'\\nShape of Y_crude: {Y_crude.shape}')\n",
    "\n",
    "del df_descriptor_gross1_atomic\n",
    "del df_target_gross1_atomic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7091485d",
   "metadata": {},
   "source": [
    "Here I implemented some classical ML models from `sklearn`:\n",
    "\n",
    "* Ridge regression\n",
    "* Support vector machine\n",
    "* Linear support vector machine\n",
    "* Random forest\n",
    "* Extra trees\n",
    "* Adaptive boosting\n",
    "* Gradient boosting\n",
    "* k-nearest neighbors\n",
    "* Dummy (if one can't beat this, then our model is wrong.)\n",
    "\n",
    "Note: the Dummy model from `sklearn` act as a good sanity check for our ML studies. If our models does not perform significantly better than the equivalent Dummy models, something is wrong in our model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b544586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb2ae4",
   "metadata": {},
   "source": [
    "In addition, we define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f83be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_model(model_name):\n",
    "    model = model_name()\n",
    "    return model\n",
    "\n",
    "def fit_model(model, X_train, y_train):\n",
    "    ti = time()\n",
    "    model = instantiate_model(model)\n",
    "    model.fit(X_train, y_train)\n",
    "    fit_time = time() - ti\n",
    "    return model, fit_time\n",
    "\n",
    "def evaluate_model(model, X, y_act):\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y_act, y_pred)\n",
    "    mae = mean_absolute_error(y_act, y_pred)\n",
    "    rmse_val = mean_squared_error(y_act, y_pred, squared=False)\n",
    "    return r2, mae, rmse_val\n",
    "\n",
    "def fit_evaluate_model(model, model_name, split, X_train, y_train, X_val, y_act_val):\n",
    "    model, fit_time = fit_model(model, X_train, y_train)\n",
    "    r2_train, mae_train, rmse_train = evaluate_model(model, X_train, y_train)\n",
    "    r2_val, mae_val, rmse_val = evaluate_model(model, X_val, y_act_val)\n",
    "    result_dict = {\n",
    "        'split': split,\n",
    "        'model_name': model_name,\n",
    "        'model_name_pretty': type(model).__name__,\n",
    "        'model_params': model.get_params(),\n",
    "        'fit_time': fit_time,\n",
    "        'r2_train': r2_train,\n",
    "        'mae_train': mae_train,\n",
    "        'rmse_train': rmse_train,\n",
    "        'r2_val': r2_val,\n",
    "        'mae_val': mae_val,\n",
    "        'rmse_val': rmse_val}\n",
    "    return model, result_dict\n",
    "\n",
    "def append_result_df(df, result_dict):\n",
    "    df_result_appended = df.append(result_dict, ignore_index=True)\n",
    "    return df_result_appended\n",
    "\n",
    "def append_model_dict(dic, model_name, model):\n",
    "    dic[model_name] = model\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bacaccd",
   "metadata": {},
   "source": [
    "Build an empty DataFrame to store model results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classics = pd.DataFrame(columns=['split',\n",
    "                                    'model_name',\n",
    "                                    'model_name_pretty',\n",
    "                                    'model_params',\n",
    "                                    'fit_time',\n",
    "                                    'r2_train',\n",
    "                                    'mae_train',\n",
    "                                    'rmse_train',\n",
    "                                    'r2_val',\n",
    "                                    'mae_val',\n",
    "                                    'rmse_val'])\n",
    "df_classics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e5d31",
   "metadata": {},
   "source": [
    "## Define the models\n",
    "\n",
    "Here, I instantiated several classical machine learning models for use.\n",
    "I have not tuned the hyperparameters of the model. And default parametes are used here.\n",
    "Hyper parameters tuning using `Grid search` will be the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fe26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary of model names\n",
    "classic_model_names = OrderedDict({\n",
    "    'dumr': DummyRegressor,\n",
    "    'rr': Ridge,\n",
    "    'abr': AdaBoostRegressor,\n",
    "    'gbr': GradientBoostingRegressor,\n",
    "    'rfr': RandomForestRegressor,\n",
    "    'etr': ExtraTreesRegressor,\n",
    "    'svr': SVR,\n",
    "    'lsvr': LinearSVR,\n",
    "    'knr': KNeighborsRegressor,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036dedf",
   "metadata": {},
   "source": [
    "## Instantiate and fit the models\n",
    "\n",
    "Now, we can fit the ML models.\n",
    "\n",
    "We will loop through each of the models listed above. For each of the models, we will:\n",
    "* instantiate the model (`with default parameters`)\n",
    "* fit the model using the training data\n",
    "* use the fitted model to generate predictions from the validation data\n",
    "* evaluate the performance of the model using the predictions\n",
    "* store the results in a DataFrame for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa0cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_act_mine(Y_act_train, Y_pred_train, Y_act, Y_pred, model, path, scale, prop, cord_list, val):\n",
    "    \n",
    "    # Setting plotting attributes\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "    fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "    fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "    fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "    \n",
    "    # Plotting\n",
    "    plot = plt.figure(figsize=(6,6))\n",
    "    \n",
    "    #print(model_name)\n",
    "    #print(model.__name__)\n",
    "    #print(model)\n",
    "    \n",
    "    #raise ValueError('Testing going on')\n",
    "    \n",
    "    if val: \n",
    "        #print('Plotting a plot for Train and Validation set')\n",
    "        \n",
    "        # Finding Maximum and minimum for straight line graph\n",
    "        \n",
    "        xy_max = np.max([np.max(Y_act_train), np.max(Y_pred_train)])\n",
    "        xy_min = np.min([np.min(Y_act_train), np.min(Y_pred_train)])\n",
    "        \n",
    "        plt.scatter(Y_act, Y_pred, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "                    label='Validation set')\n",
    "        \n",
    "        plt.scatter(Y_act_train, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "                    label='Train set')\n",
    "        \n",
    "        plt.plot([xy_min, xy_max], [xy_min,xy_max], color='black', linestyle='--')\n",
    "        \n",
    "        #plt.title(f'{type(model).__name__}, r2: {r2_score(act, pred):0.4f}')\n",
    "        plt.title(f'{type(model).__name__} model for \\ntrain and validation set ({scale})',\n",
    "                  fontdict=fontdict_t, color='black')\n",
    "        plt.axis('scaled')\n",
    "        \n",
    "        plt.xlabel(f'GCMC simulated {prop}', fontdict=fontdict_x)\n",
    "        plt.ylabel(f'ML Predicted {prop}', fontdict=fontdict_y)\n",
    "        plt.legend(loc='upper left')\n",
    "        \n",
    "        plt.text(cord_list[0], cord_list[1], str('Train     Validation'), weight='bold', horizontalalignment='left', \n",
    "                 size='medium', color='black', fontsize=10)\n",
    "\n",
    "        plt.text(cord_list[2], cord_list[3], str('$\\mathregular{R^2:}$ ') + '{:.3f}'.format(r2_score(Y_act_train, Y_pred_train))\n",
    "                 + str('   ') + '{:.3f}'.format(r2_score(Y_act, Y_pred)), weight='bold', \n",
    "                 horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "        plt.text(cord_list[4], cord_list[5], str('$\\mathregular{MAE:}$ ') + '{:.3f}'.format(mean_absolute_error(Y_act_train, Y_pred_train)) \n",
    "                 + str('   ') + '{:.3f}'.format(mean_absolute_error(Y_act, Y_pred)), weight='bold', \n",
    "                 horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "        \n",
    "        plt.text(cord_list[6], cord_list[7], str('$\\mathregular{RMSE:}$ ') + '{:.3f}'.format(mean_squared_error(Y_act_train, Y_pred_train, squared = False)) \n",
    "                 + str('   ') + '{:.3f}'.format(mean_squared_error(Y_act, Y_pred, squared = False)), weight='bold', \n",
    "                 horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "        \n",
    "        train_val_path = path + '/' +  '1_train_val' + '/' + str(model.__name__)\n",
    "        \n",
    "        #print (path)\n",
    "        #print(str(model.__name__))\n",
    "        \n",
    "        plt.savefig(train_val_path, dpi=300)\n",
    "        \n",
    "        #raise ValueError('Testing going on val = true!!')\n",
    "        \n",
    "        return plot\n",
    "    \n",
    "    else:\n",
    "        #print('Plotting a plot for Train and Test set')\n",
    "        #print('Note that here the train set is combination of train and validation set')\n",
    "        \n",
    "        # Finding Maximum and minimum for straight line graph\n",
    "        \n",
    "        xy_max = np.max([np.max(Y_act_train), np.max(Y_pred_train)])\n",
    "        xy_min = np.min([np.min(Y_act_train), np.min(Y_pred_train)])\n",
    "        \n",
    "        plt.scatter(Y_act, Y_pred, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, label='Test set')\n",
    "        \n",
    "        plt.scatter(Y_act_train, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "                    label='Train set')\n",
    "        \n",
    "        plt.plot([xy_min, xy_max], [xy_min,xy_max], color='black', linestyle='--')\n",
    "        \n",
    "        #plt.title(f'{type(model).__name__}, r2: {r2_score(act, pred):0.4f}')\n",
    "        plt.title(f'{type(model).__name__} model for \\ntrain and test set ({scale})',\n",
    "                  fontdict=fontdict_t, color='black')\n",
    "        plt.axis('scaled')\n",
    "        \n",
    "        plt.xlabel(f'GCMC simulated {prop}', fontdict=fontdict_x)\n",
    "        plt.ylabel(f'ML Predicted {prop}', fontdict=fontdict_y)\n",
    "        plt.legend(loc='upper left')\n",
    "        \n",
    "        plt.text(cord_list[0], cord_list[1], str('Train     Test'), weight='bold', horizontalalignment='left', \n",
    "                 size='medium', color='black', fontsize=10)\n",
    "\n",
    "        plt.text(cord_list[2], cord_list[3], str('$\\mathregular{R^2:}$ ') + '{:.3f}'.format(r2_score(Y_act_train, Y_pred_train))\n",
    "                 + str('   ') + '{:.3f}'.format(r2_score(Y_act, Y_pred)), weight='bold', \n",
    "                 horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "        plt.text(cord_list[4], cord_list[5], str('$\\mathregular{MAE:}$ ') + '{:.3f}'.format(mean_absolute_error(Y_act_train, Y_pred_train)) \n",
    "                 + str('   ') + '{:.3f}'.format(mean_absolute_error(Y_act, Y_pred)), weight='bold', \n",
    "                 horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "        \n",
    "        plt.text(cord_list[6], cord_list[7], str('$\\mathregular{RMSE:}$ ') + '{:.3f}'.format(mean_squared_error(Y_act_train, Y_pred_train, squared = False)) \n",
    "                 + str('   ') + '{:.3f}'.format(mean_squared_error(Y_act, Y_pred, squared = False)), weight='bold', \n",
    "                 horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "        \n",
    "        train_test_path = path + '/' + '2_train_test' + '/' + str(model.__name__)\n",
    "        \n",
    "        #print (path)\n",
    "        #print(str(type(model).__name__))\n",
    "        \n",
    "        plt.savefig(train_test_path, dpi=300)\n",
    "        \n",
    "        #raise ValueError('Testing going on val = true!!')\n",
    "        \n",
    "        return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4ca01",
   "metadata": {},
   "source": [
    "# Creating validation set and using the same validation set for all the random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_val_crude, Y, Y_val_crude = train_test_split(X_crude, Y_crude, test_size=0.32, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a dictionary to store the model objects\n",
    "classic_models = OrderedDict()\n",
    "\n",
    "# Keep track of elapsed time\n",
    "ti = time()\n",
    "\n",
    "# base path\n",
    "base_path = os.path.join(PATH, '../output/best_model/')\n",
    "\n",
    "# Mixture status\n",
    "mixture_status = \"1_Propane/\"\n",
    "#mixture_status = \"2_Ethane/\"\n",
    "\n",
    "# Which Property is used as target variable\n",
    "property_status = \"1_selectivity/\"\n",
    "#property_status = \"2_uptake_paraffin/\"\n",
    "#property_status = \"3_uptake_olefin/\"\n",
    "#property_status = \"4_TSN/\"\n",
    "\n",
    "# Whether atomic features are used or RACs are used\n",
    "feature_status = \"1_Atomic/\"\n",
    "#feature_status = \"2_RACs/\"\n",
    "\n",
    "# Combined path\n",
    "comb_path = base_path + feature_status + property_status + mixture_status\n",
    "\n",
    "# A dataframe to get the average r2 for all the splits of all the models\n",
    "df_average = pd.DataFrame(columns=['model_name',\n",
    "                                   'model_name_pretty',\n",
    "                                   '<r2_train>',\n",
    "                                   '<MAE_train>',\n",
    "                                   '<RMSE_train>',\n",
    "                                   '<r2_val>',\n",
    "                                   '<MAE_val>',\n",
    "                                   '<RMSE_val>',\n",
    "                                   '<r2_new_train>',\n",
    "                                   '<MAE_new_train>',\n",
    "                                   '<RMSE_new_train>',\n",
    "                                   '<r2_test>',\n",
    "                                   '<MAE_test>',\n",
    "                                   '<RMSE_test>'])\n",
    "\n",
    "\n",
    "# Loop through each model type, fit and predict, and evaluate and store results\n",
    "for model_name_temp, model_temp in classic_model_names.items():\n",
    "    #print(model_name)\n",
    "    #print(model.__name__)\n",
    "    #print(model)\n",
    "    #splits = range(10)\n",
    "    \n",
    "    # Model is selected\n",
    "    \n",
    "    splits = [41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
    "    \n",
    "    df_classics_val = pd.DataFrame(columns=['split',\n",
    "                                    'model_name',\n",
    "                                    'model_name_pretty',\n",
    "                                    'model_params',\n",
    "                                    'fit_time',\n",
    "                                    'r2_train',\n",
    "                                    'mae_train',\n",
    "                                    'rmse_train',\n",
    "                                    'r2_val',\n",
    "                                    'mae_val',\n",
    "                                    'rmse_val'])\n",
    "    \n",
    "    df_classics_test = df_classics_val\n",
    "    \n",
    "    for split in splits:\n",
    "        \n",
    "        # Random splits for the model selected\n",
    "        \n",
    "        model_name = model_name_temp\n",
    "        model      = model_temp\n",
    "        \n",
    "        #print('----------------')\n",
    "        #print(model_name)\n",
    "        #print(model.__name__)\n",
    "        #print(model)\n",
    "        #print('----------------')\n",
    "        \n",
    "        print(f'Fitting and evaluating model {model_name}: {model.__name__} for random seed of {split}')\n",
    "        #print(f'Fitting and evaluating model {model_name} for random seed of {split}')\n",
    "        \n",
    "        # Creating the test train split\n",
    "        \n",
    "        X_train_crude, X_test_crude, Y_train_crude, Y_test_crude = train_test_split(X, Y, test_size=0.294, random_state=split)\n",
    "\n",
    "        #-----------------------------------------------------------------------------------------------------#\n",
    "        ## For Learning curve\n",
    "        \n",
    "        #print(f'\\n X_train is :\\n\\n {X_train_crude}\\n')\n",
    "        #print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "        \n",
    "        #print(f'\\n X_val is :\\n\\n {X_val_crude}\\n')\n",
    "        #print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "        \n",
    "        #print(f'\\n X_test is :\\n\\n {X_test_crude}\\n')\n",
    "        #print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "        #raise ValueError('Testing going on!!')\n",
    "        \n",
    "#************************************************************************************************************#        \n",
    "        # Scaling the data\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        X_train_scaled = scaler.fit_transform(X_train_crude)\n",
    "        X_val_scaled   = scaler.transform(X_val_crude)\n",
    "        X_test_scaled  = scaler.transform(X_test_crude)\n",
    "        \n",
    "        # Normalizing the unscaled data\n",
    "        norm = MinMaxScaler().fit(X_train_crude)\n",
    "\n",
    "        X_train_norm  = norm.transform(X_train_crude)\n",
    "        X_val_norm    = norm.transform(X_val_crude)\n",
    "        X_test_norm   = norm.transform(X_test_crude)\n",
    "        \n",
    "        # Normalizing the scaled data\n",
    "        norm_scaled         = MinMaxScaler().fit(X_train_scaled)\n",
    "\n",
    "        X_train_scaled_norm = norm_scaled.transform(X_train_scaled)\n",
    "        X_val_scaled_norm   = norm_scaled.transform(X_val_scaled)\n",
    "        X_test_scaled_norm  = norm_scaled.transform(X_test_scaled)\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "        \n",
    "#***********************************************************************************************************#        \n",
    "        \n",
    "        ## Uncomment when model has to be trained on crude data\n",
    "        #X_train = X_train_crude\n",
    "        #X_val   = X_val_crude\n",
    "        #X_test  = X_test_crude\n",
    "\n",
    "        ## Uncomment when model has to be trained on scaled data\n",
    "\n",
    "        #X_train = X_train_scaled\n",
    "        #X_val   = X_val_scaled\n",
    "        #X_test  = X_test_scaled\n",
    "\n",
    "        ## Uncomment when model has to be trained on normalised data\n",
    "        #X_train = X_train_norm\n",
    "        #X_val   = X_val_norm\n",
    "        #X_test  = X_test_norm\n",
    "\n",
    "        ## Uncomment when model has to be trained on scaled_normalised  data\n",
    "        X_train  = X_train_scaled_norm\n",
    "        X_val    = X_val_scaled_norm\n",
    "        X_test   = X_test_scaled_norm\n",
    "        \n",
    "#***********************************************************************************************************# \n",
    "\n",
    "#***********************************************************************************************************#\n",
    "        \n",
    "        # Target Y is neigther scaled nor normalized\n",
    "    \n",
    "        # If index is 0 then, propane / ethane uptake (mol/kg)  \n",
    "        # If index is 1 then, selectivity\n",
    "        # If index is 2 then, TSN\n",
    "        # If index is 3 then, propylene / ethylene uptake (mol/kg)\n",
    "\n",
    "        i = 1\n",
    "        \n",
    "        Y_target_train = Y_train_crude.iloc[:,i]\n",
    "        Y_target_test  = Y_test_crude.iloc[:,i]\n",
    "        Y_target_val   = Y_val_crude.iloc[:,i]\n",
    "        \n",
    "        # Note feature status = atomic or RAC does not matter as cord_list does not change\n",
    "               \n",
    "        #--------------------------------------------------------------------------------------------------#\n",
    "        # Propane + {property} + {atomic / RAC (doesn't matter)} + including + scaled + normalized #\n",
    "        \n",
    "        elif (i == 1 and mixture_status == \"1_Propane/\" and property_status == \"1_selectivity/\"):\n",
    "            print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"$S_{C_{3}H_{8}/C_{3}H_{6}}$\"\n",
    "            cord_list = [1.75, 1.20, 1.65, 1.13, 1.59, 1.08, 1.56, 1.03]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "\n",
    "        elif (i == 0 and mixture_status == \"1_Propane/\" and property_status == \"2_uptake_paraffin/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"$N_{C_{3}H_{8}}$\"\n",
    "            cord_list = [1.0, 0.3, 0.85, 0.2, 0.78, 0.13, 0.74, 0.06]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "        \n",
    "        elif (i == 3 and mixture_status == \"1_Propane/\" and property_status == \"3_uptake_olefin/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"$N_{C_{3}H_{6}}$\"\n",
    "            cord_list = [4.0, 1.3, 3.4, 1.0, 3.1, 0.7, 2.9, 0.4]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "            \n",
    "        elif (i == 2 and mixture_status == \"1_Propane/\" and property_status == \"4_TSN/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"TSN\"\n",
    "            cord_list = [0.13, 0.04, 0.110, 0.025, 0.099, 0.013, 0.094, 0.001]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "            \n",
    "        #--------------------------------------------------------------------------------------------------#\n",
    "        \n",
    "        #--------------------------------------------------------------------------------------------------#\n",
    "        # Ethane + {property} + {atomic / RAC (doesn't matter)} + including + scaled + normalized #\n",
    "        \n",
    "        elif (i == 1 and mixture_status == \"2_Ethane/\" and property_status == \"1_selectivity/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"$S_{C_{2}H_{6}/C_{2}H_{4}}$\"\n",
    "            cord_list = [2.25, 1.2, 2.06, 1.05, 1.95, 0.92, 1.89, 0.79]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "\n",
    "        elif (i == 0 and mixture_status == \"2_Ethane/\" and property_status == \"2_uptake_paraffin/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"$N_{C_{2}H_{6}}$\"\n",
    "            cord_list = [0.40, 0.1, 0.35, 0.06, 0.32, 0.03, 0.305, 0.00]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "        \n",
    "        elif (i == 3 and mixture_status == \"2_Ethane/\" and property_status == \"3_uptake_olefin/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"$N_{C_{2}H_{4}}$\"\n",
    "            cord_list = [1.75, 0.5, 1.5, 0.3, 1.38, 0.15, 1.32, 0.0]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "            \n",
    "        elif (i == 2 and mixture_status == \"2_Ethane/\" and property_status == \"4_TSN/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"TSN\"\n",
    "            cord_list = [0.1, 0.040, 0.085, 0.025, 0.077, 0.013, 0.073, 0.001]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "        else :\n",
    "            raise ValueError('Combinations are wrong')\n",
    "        \n",
    "        #--------------------------------------------------------------------------------------------------#\n",
    "        \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "        #print('************')\n",
    "        #print(model_name)\n",
    "        #print(model.__name__)\n",
    "        #print(model)\n",
    "        #print('************')\n",
    "        \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "# Evaluating model performance on train and same validation set for different models\n",
    "\n",
    "        model_val, result_dict_val = fit_evaluate_model(model, model_name, split, X_train, Y_target_train, X_val, Y_target_val)\n",
    "        \n",
    "        df_classics_val = append_result_df(df_classics_val, result_dict_val)\n",
    "        \n",
    "        Y_act_train  = Y_target_train\n",
    "        Y_pred_train = model_val.predict(X_train)\n",
    "        \n",
    "        Y_act_val  = Y_target_val\n",
    "        Y_pred_val = model_val.predict(X_val)\n",
    "        \n",
    "        model_performance_path_val = comb_path  + str(model_name_temp) + '/' + 'split_' + str(split)\n",
    "        \n",
    "        plot = plot_pred_act_mine(Y_act_train, Y_pred_train, Y_act_val, Y_pred_val, model, model_performance_path_val, scale, prop, cord_list, val=True)\n",
    "        #raise ValueError('Testing going on!!')\n",
    "        \n",
    "        del model_val\n",
    "        del Y_act_train, Y_pred_train, Y_act_val, Y_pred_val\n",
    "        del model_performance_path_val\n",
    "        \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "\n",
    "# Evaluating model performance on new train set and test set for different models\n",
    "# Actually this process should be done on the best model selected in previous step\n",
    "# However, I am doing predictions on test set for all the models. This is done only for analysis purposes.\n",
    "\n",
    "        X_train_new = np.concatenate((X_train, X_val), axis=0)\n",
    "        Y_train_new = pd.concat((Y_target_train, Y_target_val), axis=0)\n",
    "        \n",
    "        model_test, result_dict_test = fit_evaluate_model(model, model_name, split, X_train_new, Y_train_new, X_test, Y_target_test)\n",
    "        \n",
    "        df_classics_test = append_result_df(df_classics_test, result_dict_test)\n",
    "        \n",
    "        Y_act_train  = Y_train_new\n",
    "        Y_pred_train = model_test.predict(X_train_new)\n",
    "        \n",
    "        Y_act_test  = Y_target_test\n",
    "        Y_pred_test = model_test.predict(X_test)\n",
    "        \n",
    "        model_performance_path_test = comb_path  + str(model_name_temp) + '/' + 'split_' + str(split)\n",
    "        \n",
    "        plot = plot_pred_act_mine(Y_act_train, Y_pred_train, Y_act_test, Y_pred_test, model, model_performance_path_test, scale, prop, cord_list, val=False)\n",
    "        \n",
    "        del X_train_new, Y_train_new\n",
    "        del model_test, Y_act_train, Y_pred_train, Y_act_test, Y_pred_test\n",
    "        del model_performance_path_test\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "        \n",
    "        del model_name, model\n",
    "        #del X, X_test_crude, Y, Y_test_crude\n",
    "        del X_test_crude, Y_test_crude\n",
    "        #del X_train_crude, X_val_crude, Y_train_crude, Y_val_crude\n",
    "        del X_train_crude, Y_train_crude\n",
    "        del scaler, X_train_scaled, X_val_scaled, X_test_scaled, \n",
    "        del norm, X_train_norm, X_val_norm, X_test_norm\n",
    "        del norm_scaled, X_train_scaled_norm, X_val_scaled_norm, X_test_scaled_norm\n",
    "        del X_train, X_val, X_test\n",
    "        del Y_target_train, Y_target_test, Y_target_val\n",
    "        \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "\n",
    "    #raise ValueError('Testing going on!!')\n",
    "    \n",
    "    df_classics_val['split'] = df_classics_val['split'].astype(int)\n",
    "    \n",
    "    print(f'\\n df_classics_val for model {model_temp.__name__} is :\\n\\n {df_classics_val}\\n')\n",
    "    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    split_stat_path_val = comb_path  + str(model_name_temp) + '/' + str(model_name_temp) + '_' +'train_val' + '.csv'\n",
    "    \n",
    "    #print(split_stat_path_val)\n",
    "    \n",
    "    df_classics_val.to_csv(split_stat_path_val, index=False)\n",
    "    \n",
    "    #raise ValueError('Testing going on!!')\n",
    "    \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "    \n",
    "    # Print the average R2, MAE and RMSE for all the splits of a particular model for train set of train_val\n",
    "    \n",
    "    avg_r2_train   = df_classics_val['r2_train'].mean()\n",
    "    avg_mae_train  = df_classics_val['mae_train'].mean()\n",
    "    avg_rmse_train = df_classics_val['rmse_train'].mean()\n",
    "    \n",
    "    print(f'Average train r2 for train-val set for model {model_temp.__name__} is       : {avg_r2_train:0.4f}')\n",
    "    print(f'Average train MAE for train-val set for for model {model_temp.__name__} is  : {avg_mae_train:0.4f}')\n",
    "    print(f'Average train RMSE for train-val set for for model {model_temp.__name__} is : {avg_rmse_train:0.4f}')\n",
    "    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    # Print the average R2, MAE and RMSE for all the splits of a particular model for validation set of train_val\n",
    "    \n",
    "    avg_r2_val   = df_classics_val['r2_val'].mean()\n",
    "    avg_mae_val  = df_classics_val['mae_val'].mean()\n",
    "    avg_rmse_val = df_classics_val['rmse_val'].mean()\n",
    "\n",
    "    print(f'Average validation r2 for train-val set for model {model_temp.__name__} is       : {avg_r2_val:0.4f}')\n",
    "    print(f'Average validation MAE for train-val set for for model {model_temp.__name__} is  : {avg_mae_val:0.4f}')\n",
    "    print(f'Average validation RMSE for train-val set for for model {model_temp.__name__} is : {avg_rmse_val:0.4f}')\n",
    "    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "\n",
    "    \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "    \n",
    "# Note here train = new_train and val = test\n",
    "\n",
    "    df_classics_test['split'] = df_classics_test['split'].astype(int)\n",
    "    \n",
    "    #print(f'\\n df_classics_test for model {model_temp.__name__} is :\\n\\n {df_classics_test}\\n')\n",
    "    #print('\\n------------------------------------------------------------\\n')\n",
    "    \n",
    "    split_stat_path_test = comb_path  + str(model_name_temp) + '/' + str(model_name_temp) + '_' + 'train_test' + '.csv'\n",
    "    \n",
    "    #print(split_stat_path_test)\n",
    "    \n",
    "    df_classics_test.to_csv(split_stat_path_test, index=False)\n",
    "    \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "    \n",
    "    # Print the average R2, MAE and RMSE for all the splits of a particular model for new train_set of for train_val\n",
    "    \n",
    "    avg_r2_new_train   = df_classics_test['r2_train'].mean() # Note here train = new_train and val = test\n",
    "    avg_mae_new_train  = df_classics_test['mae_train'].mean() # Note here train = new_train and val = test \n",
    "    avg_rmse_new_train = df_classics_test['rmse_train'].mean() # Note here train = new_train and val = test \n",
    "\n",
    "    print(f'Average new_train r2 for train-test set for model {model_temp.__name__} is   : {avg_r2_new_train:0.4f}')\n",
    "    print(f'Average new_train MAE for train-test set for model {model_temp.__name__} is  : {avg_mae_new_train:0.4f}')\n",
    "    print(f'Average new_train RMSE for train-test set for model {model_temp.__name__} is : {avg_rmse_new_train:0.4f}')\n",
    "    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "    \n",
    "    # Print the average R2, MAE and RMSE for all the splits of a particular model for test set for train_val\n",
    "    \n",
    "    avg_r2_test   = df_classics_test['r2_val'].mean() # Note here train = new_train and val = test\n",
    "    avg_mae_test  = df_classics_test['mae_val'].mean() # Note here train = new_train and val = test \n",
    "    avg_rmse_test = df_classics_test['rmse_val'].mean() # Note here train = new_train and val = test \n",
    "\n",
    "    print(f'Average validation r2 for train-test set for model {model_temp.__name__} is   : {avg_r2_test:0.4f}')\n",
    "    print(f'Average validation MAE for train-test set for model {model_temp.__name__} is  : {avg_mae_test:0.4f}')\n",
    "    print(f'Average validation RMSE for train-test set for model {model_temp.__name__} is : {avg_rmse_test:0.4f}')\n",
    "    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "    # Here we are calculating average value of R2, MAE, RMSe for all the 10 splits of a particular order\n",
    "    \n",
    "    average_dict = {\n",
    "        'model_name': model_temp.__name__,\n",
    "        'model_name_pretty': model_name_temp,\n",
    "        '<r2_train>': avg_r2_train,\n",
    "        '<MAE_train>': avg_mae_train,\n",
    "        '<RMSE_train>': avg_rmse_train,\n",
    "        '<r2_val>': avg_r2_val,\n",
    "        '<MAE_val>': avg_mae_val,\n",
    "        '<RMSE_val>': avg_rmse_val,\n",
    "        '<r2_new_train>': avg_r2_new_train,\n",
    "        '<MAE_new_train>': avg_mae_new_train,\n",
    "        '<RMSE_new_train>': avg_rmse_new_train,\n",
    "        '<r2_test>': avg_r2_test,\n",
    "        '<MAE_test>': avg_mae_test,\n",
    "        '<RMSE_test>': avg_rmse_test}\n",
    "        \n",
    "    df_average = append_result_df(df_average, average_dict)\n",
    "    \n",
    "    print(f'\\n df_average is :\\n\\n {df_average}\\n')\n",
    "    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    del avg_r2_val, avg_mae_val, avg_rmse_val\n",
    "    del avg_r2_test, avg_mae_test, avg_rmse_test\n",
    "    \n",
    "    #raise ValueError('Testing going on!!')\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "    \n",
    "    #We then plot the train and validation $r^2$ scores for each of the 10 models.\n",
    "\n",
    "    #Note the high variability in the r2_val score. In contrast, the variability in the r2_train score is comparatively lower.\n",
    "    \n",
    "    df_classics_val.plot('split', ['r2_train', 'r2_val'], kind='bar')\n",
    "    plt.title(f'Performance of {model_temp.__name__}\\nwith {len(splits)} different data splits')\n",
    "    plt.ylim((0.0, 1.0))\n",
    "    plt.ylabel('$r^2$')\n",
    "    plt.xlabel('Split #')\n",
    "    plt.legend(loc='lower right', framealpha=0.9)\n",
    "    #plt.show()\n",
    "    histo_R2_path = comb_path  + str(model_name_temp) + '/' + str(model_name_temp) + '_' + 'R2_histo.png' \n",
    "    plt.savefig(histo_R2_path, dpi=300)\n",
    "    del histo_R2_path\n",
    "    \n",
    "    df_classics_val.plot('split', ['mae_train', 'mae_val'], kind='bar')\n",
    "    plt.title(f'Performance of {model_temp.__name__}\\nwith {len(splits)} different data splits')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Split #')\n",
    "    plt.legend(loc='lower right', framealpha=0.9)\n",
    "    #plt.show()\n",
    "    histo_MAE_path = comb_path  + str(model_name_temp) + '/' + str(model_name_temp) + '_' + 'MAE_histo.png' \n",
    "    plt.savefig(histo_MAE_path, dpi=300)\n",
    "    del histo_MAE_path \n",
    "    \n",
    "    df_classics_val.plot('split', ['rmse_train', 'rmse_val'], kind='bar')\n",
    "    plt.title(f'Performance of {model_temp.__name__}\\nwith {len(splits)} different data splits')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xlabel('Split #')\n",
    "    plt.legend(loc='lower right', framealpha=0.9)\n",
    "    #plt.show()\n",
    "    histo_RMSE_path = comb_path  + str(model_name_temp) + '/' + str(model_name_temp) + '_' + 'RMSE_histo.png' \n",
    "    plt.savefig(histo_RMSE_path, dpi=300)\n",
    "    del histo_RMSE_path\n",
    "    \n",
    "    #a = df_classics_val\n",
    "    \n",
    "    del df_classics_val, df_classics_test\n",
    "\n",
    "    #raise ValueError('Testing going on!!')\n",
    "    \n",
    "#***********************************************************************************************************#\n",
    "    \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "# Sort in order of decreasing validation r2 score\n",
    "\n",
    "print(f'\\n df_average before sorting is :\\n\\n {df_average}\\n')\n",
    "print('\\n--------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "df_average = df_average.sort_values('<r2_val>', ascending=False, ignore_index=True)\n",
    "\n",
    "print(f'\\n df_average after sorting is :\\n\\n {df_average}\\n')\n",
    "print('\\n--------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "# Saving the sorted df_average\n",
    "\n",
    "df_average_path = comb_path + 'sorted_average_scores.csv'\n",
    "\n",
    "df_average.to_csv(df_average_path, index=False)\n",
    "\n",
    "del df_average_path\n",
    "\n",
    "# Find the best-performing model that we have tested\n",
    "best_row = df_average.iloc[0, :].copy()\n",
    "\n",
    "# Get the model type and model parameters\n",
    "best_model    = best_row['model_name']\n",
    "best_avg_r2   = best_row['<r2_val>']\n",
    "#best_avg_mae  = best_row['<MAE_val>']\n",
    "#best_avg_rmse = best_row['<RMSE_val>']\n",
    "\n",
    "\n",
    "print(f'\\n The best model is {best_model} with an average r2 of {best_avg_r2}\\n')\n",
    "print('\\n--------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "#model_params = best_row['model_params']\n",
    "\n",
    "# Instantiate the model again using the parameters\n",
    "#model = classic_model_names[model_name](**model_params)\n",
    "#print(model)\n",
    "\n",
    "print('\\n--------------------------------------------------------------------------------------------------\\n')\n",
    "print('------------------------------------------------------------')\n",
    "print(style.BOLD + '\\n Options for this run are :' + style.END)\n",
    "print(f'\\nFeature status : {feature_status}')\n",
    "print(f'\\nProperty_status: {property_status}')\n",
    "print(f'\\nMixture_status : {mixture_status}')\n",
    "print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError('Testing going on!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70735a79",
   "metadata": {},
   "source": [
    "## Best models\n",
    "\n",
    "\n",
    "1. For Propane + **Selectivity** + atomic + excluding + scaled + normalized best model is: `rfr with <r2_val> = 0.471 and <r2_test> = 0.438.` 2nd best model is `GradientBoostingRegressor with <r2_val> = 0.449 and <r2_test> = 0.427` (done) **(RAC inferior)**\n",
    "2. For Propane + **propane_uptake** + atomic + excluding + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.818 and <r2_test> = 0.843.` 2nd best model is `SVR with <r2_val> = 0.794217 and <r2_test> = 0.821` (done)**(RAC inferior)**\n",
    "3. For Propane + **propylene_uptake** + atomic + excluding + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.820 and <r2_test> = 0.849.` 2nd best model is `SVR with <r2_val> = 0.805 and <r2_test> = 0.818` (done) **(RAC inferior)**\n",
    "4. For Propane + **TSN_PP** + atomic + excluding + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.665 and <r2_test> = 0.708.` 2nd best model is `GradientBoostingRegressor with <r2_val> = 0.624 and <r2_test> = 0.694` **(RAC inferior)**\n",
    "\n",
    "\n",
    "5. For **Ethane** + **Selectivity** + atomic + excluding + scaled + normalized best model is: `RandomForestRegressor with <r2_val> = 0.437 and <r2_test> = 0.438.` 2nd best model is `ExtraTreesRegressor with <r2_val> = 0.432 and <r2_test> = 0.456` **(RAC inferior)**\n",
    "6.For **Ethane** + **Ethane_uptake** + atomic + excluding + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.785 and <r2_test> = 0.797.` 2nd best model is `GradientBoostingRegressor with <r2_val> = 0.781 and <r2_test> = 0.779` **(RAC inferior)**\n",
    "7.For **Ethane** + **Ethylene_uptake** + atomic + excluding + scaled + normalized best model is: `SVR with <r2_val> = 0.764 and <r2_test> = 0.776.` 2nd best model is `ExtraTreesRegressor with <r2_val> = 0.762 and <r2_test> = 0.783.' **(RAC inferior)**\n",
    "8. For **Ethane** + **TSN_EE** + atomic + excluding + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.748 and <r2_test> = 0.732.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.727 and <r2_test> = 0.717` **(RAC inferior)**\n",
    "\n",
    "\n",
    "9. For Propane + **Selectivity** + atomic + **including** + scaled + normalized best model is: `rfr with <r2_val> = 0.554 and <r2_test> = 0.571.` 2nd best model is `ExtraTreesRegressor with <r2_val> = 0.553 and <r2_test> = 0.568` (still running) **(RAC inferior)**\n",
    "10. For Propane + **Propane_uptake** + atomic + **including** + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.904 and <r2_test> = 0.919.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.889 and <r2_test> = 0.907` **(RAC inferior)**\n",
    "11. For Propane + **Proylene_uptake** + atomic + **including** + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.900 and <r2_test> = 0.917.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.886 and <r2_test> = 0.917` **(RAC inferior)**\n",
    "12. For Propane + **TSN** + atomic + **including** + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.858 and <r2_test> = 0.872.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.848 and <r2_test> = 0.863` **(RAC inferior)**\n",
    "\n",
    "\n",
    "13. For Ethane + **Selectivity** + atomic + **including** + scaled + normalized best model is: `RandomForestRegressor with <r2_val> = 0.518 and <r2_test> = 0.548.` 2nd best model is `ExtraTreesRegressor with <r2_val> = 0.516 and <r2_test> = 0.547` **(RAC inferior)**\n",
    "14. For Ethane + **ethane_uptake** + atomic + **including** + scaled + normalized best model is: `RandomForestRegressor with <r2_val> = 0.846 and <r2_test> = 0.878.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.833 and <r2_test> = 0.867` **(RAC inferior)**\n",
    "15. For Ethane + **ethylene_uptake** + atomic + **including** + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.847 and <r2_test> = 0.875.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.831 and <r2_test> = 0.864` **(RAC inferior)**\n",
    "16. For Ethane + **TSN** + atomic + **including** + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.793 and <r2_test> = 0.823.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.784 and <r2_test> = 0.814` **(RAC inferior)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7c548",
   "metadata": {},
   "source": [
    "## Best models\n",
    "\n",
    "1. For Propane + **Selectivity** + RACs + excluding + scaled + normalized best model is: `GradientBoostingRegressor with <r2_val> = 0.406 and <r2_test> = 0.4699.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.405 and <r2_test> = 0.455`\n",
    "2. For Propane + **propane_uptake** + RACs + excluding + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.750 and <r2_test> = 0.819.` 2nd best model is `GradientBoostingRegressor with <r2_val> = 0.738 and <r2_test> = 0.790`\n",
    "3. For Propane + **propylene_uptake** + RACs + excluding + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.762 and <r2_test> = 0.833.` 2nd best model is `GradientBoostingRegressor with <r2_val> = 0.746 and <r2_test> = 0.806.` 3rd best model is `RandomForestRegressor with <r2_val> = 0.728 and <r2_test> = 0.814.` \n",
    "4. For Propane + **TSN_PP** + RACs + excluding + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.644 and <r2_test> = 0.655 .` 2nd best model is `GradientBoostingRegressor with <r2_val> = 0.632 and <r2_test> = 0.663.` 3rd best model is `RandomForestRegressor with <r2_val> = 0.629 and <r2_test> = 0.646.`\n",
    "\n",
    "\n",
    "5. For **Ethane** + **Selectivity** + RACs + excluding + scaled + normalized best model is: `RandomForestRegressor with <r2_val> 0.419=  and <r2_test> = 0.417.` 2nd best model is `GradientBoostingRegressor with <r2_val> = 0.415 and <r2_test> = 0.394`\n",
    "6. For **Ethane** + **Ethane_uptake** + RACs + excluding + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> =  and <r2_test> = 0.777.` 2nd best model is `GradientBoostingRegressor with <r2_val> = 0.722 and <r2_test> = 0.744.` 3rd best model is `RandomForestRegressor with <r2_val> =0.713 and <r2_test> = 0.761.'\n",
    "7. For **Ethane** + **Ethylene_uptake** + RACs + excluding + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.711 and <r2_test> = 0.773.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.702 and <r2_test> = 0.752.'\n",
    "8. For **Ethane** + **TSN_EE** + RACs + excluding + scaled + normalized best model is: `GradientBoostingRegressor with <r2_val> = 0.711 and <r2_test> = 0.705.` 2nd best model is `ExtraTreesRegressor with <r2_val> = 0.688 and <r2_test> = 0.700.` 3rd best model is `RandomForestRegressor with <r2_val> = 0.684 and <r2_test> = 0.695.'\n",
    "\n",
    "\n",
    "9. For Propane + **Selectivity** + RACs + **including** + scaled + normalized best model is: `RandomForestRegressor with <r2_val> = 0.542 and <r2_test> = 0.588.` 2nd best model is `ExtraTreesRegressor with <r2_val> = 0.525 and <r2_test> = 0.568`\n",
    "10. For Propane + **Propane_uptake** + RACs + **including** + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.899 and <r2_test> = 0.918.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.874 and <r2_test> = 0.892`\n",
    "11. For Propane + **Proylene_uptake** + RACs + **including** + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.896 and <r2_test> =0.917.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.871 and <r2_test> = 0.892`\n",
    "12. For Propane + **TSN** + RACs + **including** + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.853 and <r2_test> =0.868 .` 2nd best model is `RandomForestRegressor with <r2_val> = 0.837 and <r2_test> = 0.853`\n",
    "\n",
    "\n",
    "13. For Ethane + **Selectivity** + RACs + **including** + scaled + normalized best model is: `RandomForestRegressor with <r2_val> = 0.529 and <r2_test> = 0.559.` 2nd best model is `ExtraTreesRegressor with <r2_val> = 0.516 and <r2_test> = 0.546`\n",
    "14. For Ethane + **ethane_uptake** + RACs + **including** + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.855 and <r2_test> = 0.871.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.838 and <r2_test> = 0.852.`\n",
    "15. For Ethane + **ethylene_uptake** + RACs + **including** + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.854 and <r2_test> = 0.871.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.831 and <r2_test> =0.847.`\n",
    "16. For Ethane + **TSN** + RACs + **including** + scaled + normalized best model is: `ExtraTreesRegressor with <r2_val> = 0.79 and <r2_test> = 0.819.` 2nd best model is `RandomForestRegressor with <r2_val> = 0.790 and <r2_test> = 0.807`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de15c468",
   "metadata": {},
   "source": [
    "## RFE for Property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eadc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670177e",
   "metadata": {},
   "source": [
    "X, X_val_crude, Y, Y_val_crude = train_test_split(X_crude, Y_crude, test_size=0.32, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, X_val_crude, Y, Y_val_crude = train_test_split(X_crude, Y_crude, test_size=0.30, random_state=RNG_SEED)\n",
    "X_train_crude, X_test_crude, Y_train_crude, Y_test_crude = train_test_split(X, Y, test_size=0.294, random_state=RNG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_crude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f2804",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_crude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a3c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_val_crude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0995ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_crude)\n",
    "X_val_scaled   = scaler.transform(X_val_crude)\n",
    "X_test_scaled  = scaler.transform(X_test_crude)\n",
    "\n",
    "# Normalizing the unscaled data\n",
    "norm = MinMaxScaler().fit(X_train_crude)\n",
    "\n",
    "X_train_norm  = norm.transform(X_train_crude)\n",
    "X_val_norm    = norm.transform(X_val_crude)\n",
    "X_test_norm   = norm.transform(X_test_crude)\n",
    "\n",
    "# Normalizing the scaled data\n",
    "norm_scaled         = MinMaxScaler().fit(X_train_scaled)\n",
    "\n",
    "X_train_scaled_norm = norm_scaled.transform(X_train_scaled)\n",
    "X_val_scaled_norm   = norm_scaled.transform(X_val_scaled)\n",
    "X_test_scaled_norm  = norm_scaled.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21534a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment when model has to be trained on crude data\n",
    "\n",
    "#X_train = X_train_crude\n",
    "#X_val   = X_val_crude\n",
    "#X_test  = X_test_crude\n",
    "\n",
    "## Uncomment when model has to be trained on scaled data\n",
    "\n",
    "#X_train = X_train_scaled\n",
    "#X_val   = X_val_scaled\n",
    "#X_test  = X_test_scaled\n",
    "\n",
    "## Uncomment when model has to be trained on normalised data\n",
    "#X_train = X_train_norm\n",
    "#X_val   = X_val_norm\n",
    "#X_test  = X_test_norm\n",
    "\n",
    "## Uncomment when model has to be trained on scaled_normalised  data\n",
    "X_train  = X_train_scaled_norm\n",
    "X_val    = X_val_scaled_norm\n",
    "X_test   = X_test_scaled_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Y is neigther scaled nor normalized\n",
    "\n",
    "# If index is 0 then, propane / ethane uptake (mol/kg)  \n",
    "# If index is 1 then, selectivity\n",
    "# If index is 2 then, TSN\n",
    "# If index is 3 then, propylene / ethylene uptake (mol/kg)\n",
    "\n",
    "print('------------------------------------------------------------')\n",
    "print(style.BOLD + 'Define property here :' + style.END)\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "Y_target_train = Y_train_crude.iloc[:,1]\n",
    "Y_target_test  = Y_test_crude.iloc[:,1]\n",
    "Y_target_val   = Y_val_crude.iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ca975",
   "metadata": {},
   "source": [
    "## RFE (Recursive Feature Elimination)\n",
    "1. RFE is used to recurcively eliminate the most unimportant features. To impliment RFE we need an estimator. Official documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html).\n",
    "2. Useful synmtax of RFE for me is :\n",
    "```\n",
    "object_variable = RFE(estimator(para_grid of estimator), n_features_to_select = index), for example\n",
    "sel = RFE(GradientBoostingRegressor(n_estimators=100, random_state=RNG_SEED), n_features_to_select = index)\n",
    "```\n",
    "3. The most important features will be calculated by RFE using following line:\n",
    "```\n",
    "sel.fit(X_train, Y_target_train)\n",
    "```\n",
    "4. But now the problem is that I don't know what would be the optimal features also I don't know whether RFE is working correctly or not. So to overcome this I implimented the methedology mentioned [here](https://www.youtube.com/watch?v=pcZ4YlvhSKU&list=PLMgWHV0KQiVoAIC62oOkwPGureNfTq0Lo&index=4).The code following this methedology is : from cell 24 to cell 26\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c9fdc",
   "metadata": {},
   "source": [
    "Now the question aries which estimator should we use for RFE? Here I have used 2 estimators which are GBR and RFE and both of them gives different set of most imp features. So which one should we use?. It turns out that it does not matter which estimator we use, although different estimators give different imp features but the final result is more or less same (infered from below markdown cells).\n",
    "\n",
    "Hence we should choose that estimator which gives lesser no of features else we can fix GBR as estimator and go ahed with it. Fix GBR as estimator for RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56662093",
   "metadata": {},
   "source": [
    "**(Propane + Selectivity + atomic + excluding + scaled + normalized)**\n",
    "\n",
    "**For RFE using GBR best features obtained were**\n",
    "`Index(['AVF(3)','GSA(2)','PLD(0)','N(21)','LCD(1)','H(11)','du(7)','Co(13)','Ni(12)','Mn'(17)], dtype='object') ` (10 features). The best model found was **RandomForestRegressor** and 2nd best model was found as **GradientBoostingRegressor**so we did hyper parameter optimisation we got best hyperparameters as: \n",
    "\n",
    "X_train = X_train[:,[3,2,0,21,1,11,7,13,12,17]]\n",
    "X_test  = X_test[:,[3,2,0,21,1,11,7,13,12,17]]\n",
    "X_val   = X_val[:,[3,2,0,21,1,11,7,13,12,17]]\n",
    "\n",
    "shap_columns = X_crude.columns[[3,2,0,21,1,11,7,13,12,17]]\n",
    "\n",
    "**1. GBR(RFE) + RFR(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} with a score of **0.059** on the validation set.\n",
    "\n",
    "\n",
    "The best combinations of parameters for **2nd grid search** are {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 100} with a score of **0.060** on the test set.\n",
    "\n",
    "**2. GBR(RFE) + GBR(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'learning_rate': 0.05, 'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.9} with a score of **0.059** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 8, 'n_estimators': 250, 'subsample': 0.9} with a score of **0.060** on the test set.\n",
    "\n",
    "\n",
    "**For RFE using RFR best features obtained were**\n",
    "`Index(['AVF(3)','GSA(2)','LCD(1)','metallic %(8)','PLD(0)','tdu(6)','H'(11),'O to M ratio(9)','I'(26),'Cu'(14),'Pb'(16)], dtype='object') ` (11 features)\n",
    "\n",
    "X_train = X_train[:,[3,2,1,8,0,6,11,9,26,14,16]]\n",
    "X_test  = X_test[:,[3,2,1,8,0,6,11,9,26,14,16]]\n",
    "X_val   = X_val[:,[3,2,1,8,0,6,11,9,26,14,16]]\n",
    "\n",
    "shap_columns = X_crude.columns[[3,2,1,8,0,6,11,9,26,14,16]]\n",
    "\n",
    "**3. RFR(RFE) + RFR(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid search** are {'bootstrap': False, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300} with a score of **0.059** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid search** are {'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 250} with a score of **0.059** on the test set.\n",
    "\n",
    "**4. RFR(RFE) + GBR(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 350, 'subsample': 0.9} with a score of **0.059** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'learning_rate': 0.01, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 8, 'n_estimators': 350, 'subsample': 0.8} with a score of **0.059** on the test set.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "**(Propane + propane_uptake + atomic + excluding + scaled + normalized)**\n",
    "\n",
    "**For RFE using GBR best features obtained were**\n",
    "`Index(['PLD(0)','GSA(2)','du(7)','H(11)','Zn(15)','LCD(1)','tdu(6)','F(25)','AVF(3)','GPV'(4), 'Ni(12)'], dtype='object') ` (11 features). The best model found was **ExtraTreesRegressor** and second best model is **SVR** so we did hyper parameter optimisation and we got best hyperparameters as: \n",
    "\n",
    "X_train = X_train[:,[0,2,7,11,15,1,6,25,3,4,12]]\n",
    "X_test  = X_test[:,[0,2,7,11,15,1,6,25,3,4,12]]\n",
    "X_val   = X_val[:,[0,2,7,11,15,1,6,25,3,4,12]]\n",
    "\n",
    "shap_columns = X_crude.columns[[0,2,7,11,15,1,6,25,3,4,12]]\n",
    "\n",
    "**1. GBR(RFE) + ExtraTreesRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} with a score of **0.058** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 350} with a score of **0.053** on the test set.\n",
    "\n",
    "**2. GBR(RFE) + SVR(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'C': 10.0, 'degree': 1, 'epsilon': 0.01, 'gamma': 1.0, 'kernel': 'rbf'} with a score of 0.055 on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'C': 10.0, 'degree': 3, 'epsilon': 0.01, 'gamma': 1.0, 'kernel': 'rbf'} with a score of 0.053 on the test set.\n",
    "\n",
    "\n",
    "**For RFE using RFR best features obtained were**\n",
    "`Index(['PLD(0)','LCD(1)','GSA(2)','du(7)','H(11)','Zn(15)','C'(19),'metallic %(8)','AVF(3)','GPV(4)','Cl(23)','Cu(14)'], dtype='object') ` (11 features)\n",
    "\n",
    "X_train = X_train[:,[0,1,2,7,11,15,19,8,3,4,23,14]]\n",
    "X_test  = X_test[:,[0,1,2,7,11,15,19,8,3,4,23,14]]\n",
    "X_val   = X_val[:,[0,1,2,7,11,15,19,8,3,4,23,14]]\n",
    "\n",
    "shap_columns = X_crude.columns[[0,1,2,7,11,15,19,8,3,4,23,14]]\n",
    "\n",
    "**3. RFR(RFE) + ExtraTreesRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 30, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200} with a score of **0.058** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 30, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500} with a score of **0.053** on the test set.\n",
    "\n",
    "**4. RFR(RFE) + SVR(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'C': 100.0, 'degree': 1, 'epsilon': 0.01, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.056** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'C': 10.0, 'degree': 3, 'epsilon': 0.001, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.054** on the test set.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "**(Propane + propylene_uptake + atomic + excluding + scaled + normalized)**\n",
    "\n",
    "**For RFE using GBR best features obtained were**\n",
    "`Index(['GSA(2)','du(7)','H(11)','Zn(15)','LCD(1)','Density(5)','tdu(6)','AVF(3)','S(22)','GPV'(4), 'N(21), 'Br(25)', 'Pb(16)'], dtype='object') ` (13 features). The best model found was **ExtraTreesRegressor** and second best model is **SVR** so we did hyper parameter optimisation and we got best hyperparameters as: \n",
    "\n",
    "X_train = X_train[:,[2,7,11,15,1,5,6,3,22,4,21,25,16]]\n",
    "X_test  = X_test[:,[2,7,11,15,1,5,6,3,22,4,21,25,16]]\n",
    "X_val   = X_val[:,[2,7,11,15,1,5,6,3,22,4,21,25,16]]\n",
    "\n",
    "shap_columns = X_crude.columns[[2,7,11,15,1,5,6,3,22,4,21,25,16]]\n",
    "\n",
    "**1. GBR(RFE) + ExtraTreesRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 250} with a score of **0.249** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 30, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 300} with a score of **0.226** on the validation set.\n",
    "\n",
    "\n",
    "**2. GBR(RFE) + SVR(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'C': 100.0, 'degree': 1, 'epsilon': 0.1, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.231** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'C': 10.0, 'degree': 3, 'epsilon': 0.1, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.230** on the test set.\n",
    "\n",
    "\n",
    "**For RFE using RFR best features obtained were**\n",
    "`Index(['PLD(0)','LCD(1)','GSA(2)','du(7)','H(11)','Zn(15)','C'(19),'O to M ratio (9)','AVF(3)','F(25)','Co(13)','Pb(16)'], dtype='object') ` (11 features)\n",
    "\n",
    "X_train = X_train[:,[0,1,2,7,11,15,19,9,3,25,13,16]]\n",
    "X_test  = X_test[:,[0,1,2,7,11,15,19,9,3,25,13,16]]\n",
    "X_val   = X_val[:,[0,1,2,7,11,15,19,9,3,25,13,16]]\n",
    "\n",
    "shap_columns = X_crude.columns[[0,1,2,7,11,15,19,9,3,25,13,16]]\n",
    "\n",
    "**3. RFR(RFE) + ExtraTreesRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200} with a score of **0.243** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 100} with a score of **0.222** on the test set.\n",
    "\n",
    "**4. RFR(RFE) + SVR(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'C': 100.0, 'degree': 1, 'epsilon': 0.1, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.240** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'C': 100.0, 'degree': 3, 'epsilon': 0.01, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.228** on the test set.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "**(Propane + Propane_selectivity + atomic + including + scaled + normalized)**\n",
    "\n",
    "**For RFE using GBR best features obtained were**\n",
    "`Index(['O to M ratio(9)','O(20)','Cu(14)','LCD(1)','Zn(15)','Co(13)','AVF(3)','metaiic %(8)','Cd(18)','Mn'(17)], dtype='object') ` (10 features). The best model found was **RandomForestRegressor** and second best model is **ExtraTreesRegressor** so we did hyper parameter optimisation and we got best hyperparameters as: \n",
    "\n",
    "X_train = X_train[:,[9,20,14,1,15,13,3,8,18,17]]\n",
    "X_test  = X_test[:,[9,20,14,1,15,13,3,8,18,17]]\n",
    "X_val   = X_val[:,[9,20,14,1,15,13,3,8,18,17]]\n",
    "\n",
    "shap_columns = X_crude.columns[[9,20,14,1,15,13,3,8,18,17]]\n",
    "\n",
    "**1. GBR(RFE) + and RandomForestRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': True, 'max_depth': 60, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 350} with a score of **0.052** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': True, 'max_depth': 40, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400} with a score of **0.051** on the test set.\n",
    "\n",
    "**2. GBR(RFE) + and ExtraTreesRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 350} with a score of **0.051** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': True, 'max_depth': 50, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400} with a score of **0.051** on the test set.\n",
    "\n",
    "**For RFE using RFR best features obtained were**\n",
    "`Index(['O(20)','O to M ratio(9)','AVF(3)','LCD(1)','metallic %(8)',GSA(2)','Cu'(14), 'H(11)','N(21)','Co(13)'], dtype='object') ` (10 features). The best model found was **RandomForestRegressor** and second best model is **ExtraTreesRegressor** so we did hyper parameter optimisation and we got best \n",
    "hyperparameters as: \n",
    "\n",
    "X_train = X_train[:,[20,9,3,1,8,2,14,11,21,13]]\n",
    "X_test  = X_test[:,[20,9,3,1,8,2,14,11,21,13]]\n",
    "X_val   = X_val[:,[20,9,3,1,8,2,14,11,21,13]]\n",
    "\n",
    "shap_columns = X_crude.columns[[20,9,3,1,8,2,14,11,21,13]]\n",
    "\n",
    "**3. RFR(RFE) + RandomForestRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 500} with a score of **0.051** on the validation set.\n",
    "\n",
    "The best combinations of parameters for 2nd grid search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 350} with a score of **0.050** on the test set.\n",
    "\n",
    "**4. RFR(RFE) + ExtraTreesRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 30, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 500} with a score of **0.051** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 30, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 400} with a score of **0.050** on the test set.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "**(Propane + Propane_uptake + atomic + including + scaled + normalized)**\n",
    "\n",
    "**For RFE using GBR best features obtained were**\n",
    "`Index(['du(7)','H(11)','GSA(2)','PLD(0)','Density(5)','metallic %(8)','S(22)','O to M ratio(9)','Ni(12)','Zn'(15),'N to O ratio(10),'Co(13)','Cd(18)','LCD(1)'], dtype='object') ` (14 features). The best model found was **ExtraTreesRegressor** and second best model is **RandomForestRegressor** so we did hyper parameter optimisation and we got best hyperparameters as: \n",
    "\n",
    "X_train = X_train[:,[7,11,2,0,5,8,22,9,12,15,10,13,18,1]]\n",
    "X_test  = X_test[:,[7,11,2,0,5,8,22,9,12,15,10,13,18,1]]\n",
    "X_val   = X_val[:,[7,11,2,0,5,8,22,9,12,15,10,13,18,1]]\n",
    "\n",
    "shap_columns = X_crude.columns[[7,11,2,0,5,8,22,9,12,15,10,13,18,1]]\n",
    "\n",
    "**1. GBR(RFE) + and ExtrasTreesRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 30, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500} with a score of **0.044** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 40, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500} with a score of **0.040** on the test set.\n",
    "\n",
    "\n",
    "**2. GBR(RFE) + and RandomForestRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500} with a score of **0.044** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 50, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500} with a score of **0.041** on the test set.\n",
    "\n",
    "**For RFE using RFR best features obtained were**\n",
    "`Index(['du(7)','H(11)','GSA(2)','Density(5)',PLD(0)',O(20)','metallic %(8)', 'LCD(1)','S(22)','O to M ratio(9)','Ni(12)','Co(13)'], dtype='object') ` (12 features). The best model found was **ExtraTreesRegressor** and second best model is **RandomForestRegressor** so we did hyper parameter optimisation and we got best hyperparameters as: \n",
    "\n",
    "X_train = X_train[:,[7,11,2,5,0,20,8,1,22,9,12,13]]\n",
    "X_test  = X_test[:,[7,11,2,5,0,20,8,1,22,9,12,13]]\n",
    "X_val   = X_val[:,[7,11,2,5,0,20,8,1,22,9,12,13]]\n",
    "\n",
    "shap_columns = X_crude.columns[[7,11,2,5,0,20,8,1,22,9,12,13]]\n",
    "\n",
    "**3. RFR(RFE) + ExtraTreesRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 40, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500} with a score of **0.045** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 50, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300} with a score of **0.042** on the test set.\n",
    "\n",
    "**4. RFR(RFE) + RandomForestRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 40, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500} with a score of **0.046** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 40, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500} with a score of **0.042** on the validation set.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "**(Propane + Propylene_uptake + atomic + including + scaled + normalized)**\n",
    "\n",
    "**For RFE using GBR best features obtained were**\n",
    "`Index(['du(7)','H(11)','GSA(2)','PLD(0)','metallic %(8)','O to M ratio (9)','S(22)','Ni(12)','Zn'(15), 'Co(13)','N to O ratio(10)], dtype='object') ` (11 features). The best model found was **ExtraTreesRegressor** and second best model is **RandomForestRegressor** so we did hyper parameter optimisation and we got best hyperparameters as: \n",
    "\n",
    "X_train = X_train[:,[7,11,2,0,8,9,22,12,15,13,10]]\n",
    "X_test  = X_test[:,[7,11,2,0,8,9,22,12,15,13,10]]\n",
    "X_val   = X_val[:,[7,11,2,0,8,9,22,12,15,13,10]]\n",
    "\n",
    "shap_columns = X_crude.columns[[7,11,2,0,8,9,22,12,15,13,10]]\n",
    "\n",
    "**1. GBR(RFE) + and ExtraTreesRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 30, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300} with a score of **0.186** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 40, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500} with a score of **0.173** on the test set.\n",
    "\n",
    "\n",
    "**2. GBR(RFE) + and RandomForestRegressor(best_model)**\n",
    "running in priyanka account NSM\n",
    "\n",
    "\n",
    "**For RFE using RFR best features obtained were**\n",
    "`Index(['du(7)','H(11)','GSA(2)',PLD(0)','metallic %(8)','LCD(1)','O to M ratio (9)','S(22)','Ni(12)', 'N to O ratio(10)] , dtype='object') ` (11 features). The best model found was **ExtraTreesRegressor** and second best model is **RandomForestRegressor** so we did hyper parameter optimisation and we got best hyperparameters as: \n",
    "\n",
    "X_train = X_train[:,[7,11,2,0,8,1,9,22,12,10]]\n",
    "X_test  = X_test[:,[7,11,2,0,8,1,9,22,12,10]]\n",
    "X_val   = X_val[:,[7,11,2,0,8,1,9,22,12,10]]\n",
    "\n",
    "shap_columns = X_crude.columns[[7,11,2,0,8,1,9,22,12,10]]\n",
    "\n",
    "**3. RFR(RFE) + ExtraTreesRegressor(best_model)**\n",
    "running in riya account NSM\n",
    "\n",
    "\n",
    "**4. RFR(RFE) + RandomForestRegressor(best_model)**\n",
    "running in jayasree account NSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc776469",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_normalized_original = X_train\n",
    "X_test_scaled_normalized_original  = X_test\n",
    "X_val_scaled_normalized_original   = X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9ae5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_crude.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77edcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb10ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Propane + Selectivity + atomic + excluding + scaled + normalized) + GBR\n",
    "\n",
    "X_train = X_train[:,[3,2,0,21,1,11,7,13,12,17]]\n",
    "X_test  = X_test[:,[3,2,0,21,1,11,7,13,12,17]]\n",
    "X_val   = X_val[:,[3,2,0,21,1,11,7,13,12,17]]\n",
    "\n",
    "shap_columns = X_crude.columns[[3,2,0,21,1,11,7,13,12,17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ced7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise ValueError('Testing going on!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear', random_state=42)\n",
    "\n",
    "pipe = make_pipeline(RFE(estimator=lr, step=1),\n",
    "                     KNeighborsRegressor())\n",
    "\n",
    "\n",
    "parameters = {'rfe__n_features_to_select': range(1, 13), \n",
    "              'kneighborsregressor__n_neighbors': range(1, 10) }\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=parameters, cv=10, n_jobs=-1)\n",
    "grid.fit(X_train_scaled_normalized_original, Y_target_train)\n",
    "\n",
    "print('Best params:', grid.best_params_)\n",
    "print('Best accuracy:', grid.best_score_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f15780",
   "metadata": {},
   "source": [
    "## Using pipeline to do RFE and model development at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358e2a5",
   "metadata": {},
   "source": [
    "### Note that the results should be compaired with the results of cell 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rfr_pipe = RandomForestRegressor(random_state=RNG_SEED) # Creating an estimator for RFE\n",
    "gbr_pipe = GradientBoostingRegressor(random_state=RNG_SEED) # Creating an estimator for RFE\n",
    "\n",
    "pipe = make_pipeline(RFE(estimator=gbr_pipe, step=1),\n",
    "                     RandomForestRegressor()) # Here RandomForestRegressor() this is used because best model obtained was rfr \n",
    "\n",
    "# Parameter grid for RFR and estimator of best model}\n",
    "\n",
    "\n",
    "parameters = {'rfe__n_features_to_select': range(1, 27), \n",
    "              'randomforestregressor__bootstrap': [True, False], \n",
    "              'randomforestregressor__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "              'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'], \n",
    "              'randomforestregressor__n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 150, 500],\n",
    "              'randomforestregressor__min_samples_split': [2, 4, 6, 8, 10],\n",
    "              'randomforestregressor__min_samples_leaf': [1, 2, 3, 4, 5]}\n",
    "\n",
    " \n",
    "grid = GridSearchCV(pipe, param_grid=parameters, cv=10, scoring = 'neg_mean_absolute_error', return_train_score = True, n_jobs = -1,\n",
    "                          verbose=1000)\n",
    "\n",
    "grid.fit(X_train_scaled_normalized_original, Y_target_train)\n",
    "\n",
    "# Print best parameters and Best score for train set\n",
    "\n",
    "print('Best params:', grid.best_params_)\n",
    "print('Best accuracy:', grid.best_score_)\n",
    "\n",
    "# Score on \\reduced feature set from grid search\n",
    "\n",
    "grid.best_estimator_.score(X_val_scaled_normalized_original, Y_target_val)\n",
    "\n",
    "#raise ValueError('Testing going on!!')\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "print(f'Selected Features are : \\n {features}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "print()\n",
    "\n",
    "# Here we should put the best parameters of rfr model\n",
    "# Here we want to compare R2_ral with RFE and without RFE\n",
    "# Full feature set for reference\n",
    "\n",
    "del rfr_pipe\n",
    "del parameters\n",
    "del grid\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ec11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid.best_estimator_.score(X_val_scaled_normalized_original, Y_target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8656a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"The best combinations of parameters are %s with a score of %0.3f on the validation set.\"\n",
    "#      % (grid.best_params_, -grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cv_result_selectivity = pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5851b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the results of all the hyperparameters searched\n",
    "\n",
    "##df_cv_result_selectivity = df_cv_result_selectivity.astype(float)\n",
    "#df_cv_result_selectivity.to_csv('grid_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72987296",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def run_randomForest_test(X_train, X_val, Y_train, Y_val):\n",
    "    \n",
    "    rfr = RandomForestRegressor(random_state=RNG_SEED, bootstrap=True, max_depth=10,max_features='auto',\n",
    "                                min_samples_leaf=2, min_samples_split=2, n_estimators = 50, n_jobs = 10)\n",
    "    rfr.fit(X_train, Y_train)\n",
    "    Y_pred_train = rfr.predict(X_train)\n",
    "    Y_pred_val  = rfr.predict(X_val)\n",
    "    print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train, Y_pred_train))\n",
    "    print(\"\\nR^2 score on validation set: %.3f\\n\" % r2_score(Y_val, Y_pred_val))\n",
    "    print(\"\\nMAE score on validation set: %.3f\\n\" % mean_absolute_error(Y_val, Y_pred_val))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82185fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sel = RFE(GradientBoostingRegressor(random_state=RNG_SEED), n_features_to_select = 24)\n",
    "      # Calling RFE\n",
    "\n",
    "sel.fit(X_train_scaled_normalized_original, Y_target_train) # At this point RFE has selected the $index most important features \n",
    "                                 # (where index = no of featues = variable from 1 to 29)\n",
    "\n",
    "X_train_rfe = sel.transform(X_train_scaled_normalized_original) # Say X_train has colums = 29, index = 1, then X_train_rfe will have\n",
    "X_val_rfe = sel.transform(X_val_scaled_normalized_original) # same features as selected by RFE. This is done because I wanted to select\n",
    "                                 # Only those features which perform good on validation set\n",
    "\n",
    "print('No of Selected Feature are : 24')\n",
    "\n",
    "run_randomForest_test(X_train_rfe, X_val_rfe, Y_target_train, Y_target_val) # Calculating R2 score for train and validation set\n",
    "\n",
    "features = X.columns[sel.get_support()] # printing the columns\n",
    "\n",
    "print(f'Selected Features are : \\n {features}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4ef1e",
   "metadata": {},
   "source": [
    "## Grid search for Propane selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad7ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(random_state=RNG_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d03677",
   "metadata": {},
   "source": [
    "## Best hyper parameters are:\n",
    "`The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} with a score of **0.059** on the validation set.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "param_grid = {  'bootstrap': [True, False], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None], \n",
    "              'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 150, 500],\n",
    "             'min_samples_split': [2, 4, 6, 8, 10], 'min_samples_leaf' : [1, 2, 3, 4, 5]}\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "param_grid = {  'bootstrap': [False], 'max_depth': [30], \n",
    "              'max_features': ['sqrt'], 'n_estimators': [300,100],\n",
    "             'min_samples_split': [8], 'min_samples_leaf' : [2]}\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Following RF parameters are taken from supplementry of paper titled 'Understanding the diversity of MOF'\n",
    "\n",
    "param_grid = {  'bootstrap': [True], 'max_depth': [5, 10, 20, 40], \n",
    "              'max_features': ['auto'], 'n_estimators': [50, 100, 150, 200, 250, 300],\n",
    "             'min_samples_split': [2, 4], 'min_samples_leaf' : [1,2]}\n",
    "'''\n",
    "\n",
    "# Best parameters are following\n",
    "param_grid = {  'bootstrap': [False], 'max_depth': [10], \n",
    "              'max_features': ['sqrt'], 'n_estimators': [50],\n",
    "             'min_samples_split': [2], 'min_samples_leaf' : [1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726aee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artificial mof\n",
    "\n",
    "df_artificial_mof_X = pd.read_csv('/home/varad/varad/literature/24_sauradeep/workspace/ML/input/descriptor_database/2_with_full_chemical_descriptor/10_X_artificial_mof_database_join.csv')\n",
    "df_artificial_mof_Y = pd.read_csv('/home/varad/varad/literature/24_sauradeep/workspace/ML/input/Target_property_database/artificial_mof.csv')\n",
    "\n",
    "rename_dict = {'name': 'mof', 'Di': 'LCD', 'Df': 'PLD', 'ASA(m2/gram)_1.9': 'GSA', \n",
    "               'AV_Volume_fraction_1.9': 'AVF', 'AV(cm3/gram)_1.9': 'GPV', 'density(gram_cm3)': 'Density'}\n",
    "\n",
    "df_artificial_mof_X = df_artificial_mof_X.rename(columns=rename_dict)\n",
    "\n",
    "\n",
    "columns = ['PLD', 'LCD', 'GSA', 'AVF', 'GPV', 'Density', 'total_degree_unsaturation', 'degree_unsaturation', \n",
    "           'metallic_percentage', 'O_to_Metal_ration', 'N_to_O_ratio', 'H' ,'Ni', 'Co', 'Cu', 'Zn', 'Pb', 'Mn',\n",
    "           'Cd', 'C', 'O', 'N', 'S', 'Cl', 'Br', 'F', 'I']\n",
    "\n",
    "df_artificial_mof_X = df_artificial_mof_X[columns].astype(float)\n",
    "\n",
    "X_artificial_scaled_norm   = norm_scaled.transform(df_artificial_mof_X)\n",
    "Y_target_artificial   = df_artificial_mof_Y.iloc[:,3] # load_y_artificial_mof.csv\n",
    "\n",
    "# Concatenate the train and validation datasets together\n",
    "X_train = np.concatenate((X_train, X_artificial_scaled_norm), axis=0)\n",
    "Y_target_train = pd.concat((Y_target_train, Y_target_artificial), axis=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_target_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be299466",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator = rfr, param_grid = param_grid, cv = 10,\n",
    "                           scoring = 'neg_mean_absolute_error', return_train_score = True, n_jobs = -1,\n",
    "                          verbose=1000)\n",
    "\n",
    "grid_search.fit(X_train, Y_target_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best combinations of parameters are %s with a score of %0.3f on the validation set.\"\n",
    "      % (grid_search.best_params_, -grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted Propane selectivity for all the structures\n",
    "\n",
    "Y_pred_val = grid_search.predict(X_val) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = grid_search.predict(X_train)\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "\n",
    "plt.scatter(Y_target_val, Y_pred_val, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.scatter(Y_target_train, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, label='Train set')\n",
    "\n",
    "plt.scatter(Y_target_val, Y_pred_val, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.plot([np.min(Y_target_val),np.max(Y_target_val)], \n",
    "         [np.min(Y_target_val),np.max(Y_target_val)], color='black', linestyle='--')\n",
    "\n",
    "plt.title('Performance of ML model for \\ntrain and validation set', fontdict=fontdict_t, color='black')\n",
    "\n",
    "plt.xlabel('GCMC simulated $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_x)\n",
    "plt.ylabel('ML Predicted $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_y)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "ticks = [1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0,2.1]\n",
    "plt.axes().set_xticks(ticks)\n",
    "plt.axes().set_yticks(ticks)\n",
    "\n",
    "#[1.75, 1.32, 1.7, 1.27, 1.665, 1.24, 1.65, 1.21]\n",
    "plt.text(1.75, 1.32, str('Train     Val'), weight='bold', horizontalalignment='left', size='medium', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.7, 1.27, str('$\\mathregular{R^2:}$') + '{:.3f}'.format(r2_score(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(r2_score(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.665, 1.23, str('$\\mathregular{MAE: }$') + '{:.3f}'.format(mean_absolute_error(Y_target_train, Y_pred_train)) +  \n",
    "         str('   ') + '{:.3f}'.format(mean_absolute_error(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.645, 1.19, str('$\\mathregular{RMSE: }$') + '{:.3f}'.format(mean_squared_error(Y_target_train, Y_pred_train)) +  \n",
    "         str('   ') + '{:.3f}'.format(mean_squared_error(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms.eps', format='eps')\n",
    "\n",
    "\n",
    "#print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train_new, Y_pred_train))\n",
    "#print(\"\\nMean absolute error on train set: %0.3f \\n\" %(np.abs(Y_pred_train-Y_train_new)).mean())\n",
    "\n",
    "#print(\"\\nR^2 score on test set: %.3f\\n\" % r2_score(Y_test.iloc[:,1], Y_pred_test))\n",
    "#print(\"\\nMean absolute error on val set: %0.3f\\n\" %(np.abs(Y_pred_test-Y_test.iloc[:,1])).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c912b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d884a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f7076",
   "metadata": {},
   "source": [
    "## Shap analysis on train and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829bf0a0",
   "metadata": {},
   "source": [
    "`'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} with a score of 0.059 on the validation set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del rfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(random_state=RNG_SEED, bootstrap = 'False', max_depth = 10,\n",
    "                            max_features = 'sqrt', n_estimators = 50, min_samples_split = 2,\n",
    "                            min_samples_leaf = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23549cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.fit(X_train, Y_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted Propane selectivity for all the structures\n",
    "\n",
    "Y_pred_val = rfr.predict(X_val) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = rfr.predict(X_train)\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "\n",
    "plt.scatter(Y_target_val, Y_pred_val, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.scatter(Y_target_train, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, label='Train set')\n",
    "\n",
    "plt.plot([np.min(Y_target_val),np.max(Y_target_val)], \n",
    "         [np.min(Y_target_val),np.max(Y_target_val)], color='black', linestyle='--')\n",
    "\n",
    "plt.title('Performance of ML model for \\ntrain and validation set', fontdict=fontdict_t, color='black')\n",
    "\n",
    "plt.xlabel('GCMC simulated $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_x)\n",
    "plt.ylabel('ML Predicted $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_y)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "ticks = [1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0,2.1]\n",
    "plt.axes().set_xticks(ticks)\n",
    "plt.axes().set_yticks(ticks)\n",
    "\n",
    "plt.text(1.75, 1.32, str('Train     Val'), weight='bold', horizontalalignment='left', size='medium', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.7, 1.27, str('$\\mathregular{R^2:}$') + '{:.3f}'.format(r2_score(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(r2_score(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.665, 1.23, str('$\\mathregular{MAE: }$') + '{:.3f}'.format(mean_absolute_error(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_absolute_error(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.645, 1.19, str('$\\mathregular{RMSE: }$') + '{:.3f}'.format(mean_squared_error(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_squared_error(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms.eps', format='eps')\n",
    "\n",
    "\n",
    "#print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train_new, Y_pred_train))\n",
    "#print(\"\\nMean absolute error on train set: %0.3f \\n\" %(np.abs(Y_pred_train-Y_train_new)).mean())\n",
    "\n",
    "#print(\"\\nR^2 score on test set: %.3f\\n\" % r2_score(Y_test.iloc[:,1], Y_pred_test))\n",
    "#print(\"\\nMean absolute error on val set: %0.3f\\n\" %(np.abs(Y_pred_test-Y_test.iloc[:,1])).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bade42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.TreeExplainer(rfr)\n",
    "shap_values = explainer.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train, feature_names = shap_columns, max_display = 15, show = False)\n",
    "plt.title(\"Feature importance calculated using SHAP for propane-propylene selectivity for train set\\n\", fontweight = \"bold\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2999f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1219f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a879c66",
   "metadata": {},
   "source": [
    "## SHAP for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d31d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_val, feature_names = shap_columns, max_display = 15, show = False)\n",
    "plt.title(\"Feature importance calculated using SHAP for propane-propylene selectivity for validation set\\n\", fontweight = \"bold\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef4f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "localimportance = 0.0\n",
    "for index, feature_importance in enumerate(grid_search.best_estimator_.feature_importances_):\n",
    "    if feature_importance > localimportance:\n",
    "        localimportance = feature_importance\n",
    "        print(index, feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "del rfr\n",
    "del grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e5db20",
   "metadata": {},
   "source": [
    "## Combining training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f14eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the train and validation datasets together\n",
    "X_train_new = np.concatenate((X_train, X_val), axis=0)\n",
    "Y_train_new = pd.concat((Y_target_train, Y_target_val), axis=0)\n",
    "\n",
    "A_X = X_train_new\n",
    "\n",
    "print(X_train_new.shape)\n",
    "print(Y_train_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(Y_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6843908d",
   "metadata": {},
   "source": [
    "## Retraining model on combined train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a20f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa86d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(random_state=RNG_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7256844",
   "metadata": {},
   "source": [
    "## Best hyper parameters are: \n",
    "\n",
    "`The best combinations of parameters for **2nd grid search** are {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 100} with a score of **0.060** on the test set.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a46120",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "param_grid = {  'bootstrap': [True, False], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None], \n",
    "              'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 150, 500],\n",
    "             'min_samples_split': [2, 4, 6, 8, 10], 'min_samples_leaf' : [1, 2, 3, 4, 5]}\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "param_grid = {  'bootstrap': [True], 'max_depth': [20], \n",
    "              'max_features': ['sqrt'], 'n_estimators': [300,50],\n",
    "             'min_samples_split': [6], 'min_samples_leaf' : [1]}\n",
    "'''\n",
    "\n",
    "# Following RF parameters are taken from supplementry of paper titled 'Understanding the diversity of MOF'\n",
    "\n",
    "'''\n",
    "param_grid = {  'bootstrap': [True], 'max_depth': [5, 10, 20, 40], \n",
    "              'max_features': ['auto'], 'n_estimators': [50, 100, 150, 200, 250, 300],\n",
    "             'min_samples_split': [2, 4], 'min_samples_leaf' : [1,2]}\n",
    "'''\n",
    "\n",
    "# Best hyper parameters are \n",
    "param_grid = {  'bootstrap': [True], 'max_depth': [10], \n",
    "              'max_features': ['sqrt'], 'n_estimators': [100],\n",
    "             'min_samples_split': [6], 'min_samples_leaf' : [1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd74639",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator = rfr, param_grid = param_grid, cv = 10,\n",
    "                           scoring = 'neg_mean_absolute_error', return_train_score = True, n_jobs = -1,\n",
    "                          verbose=1000)\n",
    "\n",
    "grid_search.fit(X_train_new, Y_train_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae56284",
   "metadata": {},
   "source": [
    "## Grid search results\n",
    "\n",
    "Print out the average validation errors and corresponding hyperparameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc417d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_result_selectivity = pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cv_result_selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bcdfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results of all the hyperparameters searched\n",
    "\n",
    "#df_cv_result_selectivity = df_cv_result_selectivity.astype(float)\n",
    "#df_cv_result_selectivity.to_csv('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/grid_search_results/selectivity/1_selectivity_propane_propylene_grid_search_results_excluding_oms.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f38534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_result_selectivity.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(f'\\nGrid search results:\\n\\n {df_cv_result_selectivity}\\n')\n",
    "print('\\n------------------------------------------------------------\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ca773",
   "metadata": {},
   "source": [
    "## Printing out the average validation errors and corresponding hyperparameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa175f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test = grid_search.cv_results_['mean_test_score']\n",
    "std_test = grid_search.cv_results_['std_test_score']\n",
    "\n",
    "mean_train = grid_search.cv_results_['mean_train_score']\n",
    "std_train = grid_search.cv_results_['std_train_score']\n",
    "\n",
    "'''\n",
    "for mean, std, params in zip(-means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best combinations of parameters are %s with a score of %0.3f on the validation set.\"\n",
    "      % (grid_search.best_params_, -grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted Propane selectivity for all the structures\n",
    "\n",
    "Y_pred_test = grid_search.predict(X_test) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = grid_search.predict(X_train_new)\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "\n",
    "a = Y_train_new\n",
    "b = Y_target_test\n",
    "\n",
    "plt.scatter(Y_target_test, Y_pred_test, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.scatter(Y_train_new, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, label='Train set')\n",
    "\n",
    "plt.plot([np.min(Y_target_test),np.max(Y_target_test)], \n",
    "         [np.min(Y_target_test),np.max(Y_target_test)], color='black', linestyle='--')\n",
    "\n",
    "plt.title('Performance of ML model for \\ntrain and test set', fontdict=fontdict_t, color='black')\n",
    "\n",
    "plt.xlabel('GCMC simulated $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_x)\n",
    "plt.ylabel('ML Predicted $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_y)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "ticks = [1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0,2.1]\n",
    "plt.axes().set_xticks(ticks)\n",
    "plt.axes().set_yticks(ticks)\n",
    "\n",
    "#[1.75, 1.32, 1.7, 1.27, 1.665, 1.24, 1.65, 1.21]\n",
    "\n",
    "plt.text(1.75, 1.32, str('Train     Test'), weight='bold', horizontalalignment='left', size='medium', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.7, 1.27, str('$\\mathregular{R^2:}$') + '{:.3f}'.format(r2_score(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(r2_score(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.665, 1.23, str('$\\mathregular{MAE: }$') + '{:.3f}'.format(mean_absolute_error(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_absolute_error(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.645, 1.19, str('$\\mathregular{RMSE: }$') + '{:.3f}'.format(mean_squared_error(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_squared_error(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms.eps', format='eps')\n",
    "\n",
    "\n",
    "#print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train_new, Y_pred_train))\n",
    "#print(\"\\nMean absolute error on train set: %0.3f \\n\" %(np.abs(Y_pred_train-Y_train_new)).mean())\n",
    "\n",
    "#print(\"\\nR^2 score on test set: %.3f\\n\" % r2_score(Y_test.iloc[:,1], Y_pred_test))\n",
    "#print(\"\\nMean absolute error on val set: %0.3f\\n\" %(np.abs(Y_pred_test-Y_test.iloc[:,1])).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbdc1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da12a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6393db80",
   "metadata": {},
   "source": [
    "## Feature importance of rfr model. Note that I think this is option to shap analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcafd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise ValueError('Testing going on!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d5ce9",
   "metadata": {},
   "source": [
    "## Shap analysis on train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810b2c6",
   "metadata": {},
   "source": [
    "`'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 100} with a score of 0.060 on the validation set.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5882f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del rfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22bbc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(random_state=RNG_SEED, bootstrap = 'True', max_depth = 10,\n",
    "                            max_features = 'sqrt', n_estimators = 100, min_samples_split = 6,\n",
    "                            min_samples_leaf = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ec2a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.fit(X_train_new, Y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a62235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted Propane selectivity for all the structures\n",
    "\n",
    "Y_pred_test = rfr.predict(X_test) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = rfr.predict(X_train_new)\n",
    "\n",
    "Y_pred_test_model_selectivity  = Y_pred_test\n",
    "Y_pred_train_model_selectivity = Y_pred_train\n",
    "\n",
    "X_test_model_selectivity  = X_test\n",
    "X_train_model_selectivity = X_train\n",
    "\n",
    "Y_act_test_model_selectivity  = Y_target_test\n",
    "Y_act_train_model_selectivity = Y_train_new\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "\n",
    "plt.scatter(Y_target_test, Y_pred_test, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.scatter(Y_train_new, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, label='Train set')\n",
    "\n",
    "plt.plot([np.min(Y_target_test),np.max(Y_target_test)], \n",
    "         [np.min(Y_target_test),np.max(Y_target_test)], color='black', linestyle='--')\n",
    "\n",
    "plt.title('Performance of ML model for \\ntrain and test set', fontdict=fontdict_t, color='black')\n",
    "\n",
    "plt.xlabel('GCMC simulated $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_x)\n",
    "plt.ylabel('ML Predicted $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_y)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "ticks = [1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0,2.1]\n",
    "plt.axes().set_xticks(ticks)\n",
    "plt.axes().set_yticks(ticks)\n",
    "\n",
    "#[1.75, 1.32, 1.7, 1.27, 1.665, 1.24, 1.65, 1.21]\n",
    "\n",
    "plt.text(1.75, 1.32, str('Train     Test'), weight='bold', horizontalalignment='left', size='medium', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.7, 1.27, str('$\\mathregular{R^2:}$') + '{:.3f}'.format(r2_score(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(r2_score(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.665, 1.23, str('$\\mathregular{MAE: }$') + '{:.3f}'.format(mean_absolute_error(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_absolute_error(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.645, 1.19, str('$\\mathregular{RMSE: }$') + '{:.3f}'.format(mean_squared_error(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_squared_error(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms.eps', format='eps')\n",
    "\n",
    "\n",
    "#print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train_new, Y_pred_train))\n",
    "#print(\"\\nMean absolute error on train set: %0.3f \\n\" %(np.abs(Y_pred_train-Y_train_new)).mean())\n",
    "\n",
    "#print(\"\\nR^2 score on test set: %.3f\\n\" % r2_score(Y_test.iloc[:,1], Y_pred_test))\n",
    "#print(\"\\nMean absolute error on val set: %0.3f\\n\" %(np.abs(Y_pred_test-Y_test.iloc[:,1])).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49515c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = pd.DataFrame(Y_pred_train)\n",
    "predict.to_csv('1_st_model_selectivity_train_predict.csv', index=False)\n",
    "\n",
    "predict = pd.DataFrame(Y_train_new)\n",
    "predict.to_csv('1_st_model_selectivity_train_actual.csv', index=False)\n",
    "\n",
    "predict = pd.DataFrame(Y_pred_test)\n",
    "predict.to_csv('1_st_model_selectivity_test_predict.csv', index=False)\n",
    "\n",
    "predict = pd.DataFrame(Y_target_test)\n",
    "predict.to_csv('1_st_model_selectivity_test_actual.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_pred_first_model = pd.concat((Y_pred_train, Y_pred_test), axis=0)\n",
    "#Y_act_first_model = pd.concat((Y_train_new, Y_target_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8f699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c828d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test_model_selectivity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c39ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train_model_selectivity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del shap\n",
    "\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(rfr)\n",
    "shap_values = explainer.shap_values(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train_new, feature_names = shap_columns, max_display = 15, show = False)\n",
    "plt.title(\"Feature importance calculated using SHAP for propane-propylene selectivity for train+validation set\\n\", fontweight = \"bold\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c051b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display = 15,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5810d92",
   "metadata": {},
   "source": [
    "## SHAP for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f27e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, feature_names = shap_columns, max_display = 15, show = False)\n",
    "plt.title(\"Feature importance calculated using SHAP for propane-propylene selectivity for test set\\n\", fontweight = \"bold\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8bec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "localimportance = 0.0\n",
    "for index, feature_importance in enumerate(grid_search.best_estimator_.feature_importances_):\n",
    "    if feature_importance > localimportance:\n",
    "        localimportance = feature_importance\n",
    "        print(index, feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8557fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_selectivity = grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526b2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del rfr\n",
    "del RandomForestRegressor\n",
    "del param_grid\n",
    "del grid_search\n",
    "del GridSearchCV\n",
    "del ticks\n",
    "#del mean_val\n",
    "del mean_train\n",
    "#del std_val\n",
    "#del std_train\n",
    "del Y_pred_val\n",
    "del Y_pred_train\n",
    "del localimportance\n",
    "del explainer\n",
    "del index\n",
    "del feature_importance\n",
    "del df_cv_result_selectivity\n",
    "del shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise ValueError('Testing going on!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ffdf8",
   "metadata": {},
   "source": [
    "## RFE (Recursive Feature Elimination) for propane uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc70d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment when model has to be trained on crude data\n",
    "\n",
    "#X_train = X_train_crude\n",
    "#X_val   = X_val_crude\n",
    "#X_test  = X_test_crude\n",
    "\n",
    "## Uncomment when model has to be trained on scaled data\n",
    "\n",
    "#X_train = X_train_scaled\n",
    "#X_val   = X_val_scaled\n",
    "#X_test  = X_test_scaled\n",
    "\n",
    "## Uncomment when model has to be trained on normalised data\n",
    "#X_train = X_train_norm\n",
    "#X_val   = X_val_norm\n",
    "#X_test  = X_test_norm\n",
    "\n",
    "## Uncomment when model has to be trained on scaled_normalised  data\n",
    "X_train  = X_train_scaled_norm\n",
    "X_val    = X_val_scaled_norm\n",
    "X_test   = X_test_scaled_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Y is neigther scaled nor normalized\n",
    "\n",
    "# If index is 0 then, propane / ethane uptake (mol/kg)  \n",
    "# If index is 1 then, selectivity\n",
    "# If index is 2 then, TSN\n",
    "# If index is 3 then, propylene / ethylene uptake (mol/kg)\n",
    "\n",
    "Y_target_train = Y_train_crude.iloc[:,0]\n",
    "Y_target_test  = Y_test_crude.iloc[:,0]\n",
    "Y_target_val   = Y_val_crude.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa115ef4",
   "metadata": {},
   "source": [
    "\n",
    "**(Propane + propane_uptake + atomic + excluding + scaled + normalized)**\n",
    "\n",
    "**For RFE using GBR best features obtained were**\n",
    "`Index(['PLD(0)','GSA(2)','du(7)','H(11)','Zn(15)','LCD(1)','tdu(6)','F(25)','AVF(3)','GPV'(4), 'Ni(12)'], dtype='object') ` (11 features). The best model found was **ExtraTreesRegressor** and second best model is **SVR** so we did hyper parameter optimisation and we got best hyperparameters as: \n",
    "\n",
    "X_train = X_train[:,[0,2,7,11,15,1,6,25,3,4,12]]\n",
    "X_test  = X_test[:,[0,2,7,11,15,1,6,25,3,4,12]]\n",
    "X_val   = X_val[:,[0,2,7,11,15,1,6,25,3,4,12]]\n",
    "\n",
    "shap_columns = X_crude.columns[[0,2,7,11,15,1,6,25,3,4,12]]\n",
    "\n",
    "**1. GBR(RFE) + ExtraTreesRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} with a score of **0.058** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 350} with a score of **0.053** on the test set.\n",
    "\n",
    "**2. GBR(RFE) + SVR(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'C': 10.0, 'degree': 1, 'epsilon': 0.01, 'gamma': 1.0, 'kernel': 'rbf'} with a score of 0.055 on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'C': 10.0, 'degree': 3, 'epsilon': 0.01, 'gamma': 1.0, 'kernel': 'rbf'} with a score of 0.053 on the test set.\n",
    "\n",
    "\n",
    "**For RFE using RFR best features obtained were**\n",
    "`Index(['PLD(0)','LCD(1)','GSA(2)','du(7)','H(11)','Zn(15)','C'(19),'metallic %(8)','AVF(3)','GPV(4)','Cl(23)','Cu(14)'], dtype='object') ` (11 features)\n",
    "\n",
    "X_train = X_train[:,[0,1,2,7,11,15,19,8,3,4,23,14]]\n",
    "X_test  = X_test[:,[0,1,2,7,11,15,19,8,3,4,23,14]]\n",
    "X_val   = X_val[:,[0,1,2,7,11,15,19,8,3,4,23,14]]\n",
    "\n",
    "shap_columns = X_crude.columns[[0,1,2,7,11,15,19,8,3,4,23,14]]\n",
    "\n",
    "**3. RFR(RFE) + ExtraTreesRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 30, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200} with a score of **0.058** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 30, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500} with a score of **0.053** on the test set.\n",
    "\n",
    "**4. RFR(RFE) + SVR(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'C': 100.0, 'degree': 1, 'epsilon': 0.01, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.056** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'C': 10.0, 'degree': 3, 'epsilon': 0.001, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.054** on the test set.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b580aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_crude.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Propane + propane_uptake + atomic + excluding + scaled + normalized) + GBR\n",
    "\n",
    "X_train = X_train[:,[0,2,7,11,15,1,6,25,3,4,12]]\n",
    "X_test  = X_test[:,[0,2,7,11,15,1,6,25,3,4,12]]\n",
    "X_val   = X_val[:,[0,2,7,11,15,1,6,25,3,4,12]]\n",
    "\n",
    "shap_columns = X_crude.columns[[0,2,7,11,15,1,6,25,3,4,12]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd21626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise ValueError('Testing going on!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648a81f",
   "metadata": {},
   "source": [
    "## Grid search for Propane uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44422d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "#from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "etr = ExtraTreesRegressor(random_state=RNG_SEED)\n",
    "#svr = SVR() # random_state=RNG_SEED cannot be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f221ed19",
   "metadata": {},
   "source": [
    "## Best hyper parameters are:\n",
    "\n",
    "ExtraTreesRegressor\n",
    "\n",
    "`The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} with a score of **0.058** on the validation set.`\n",
    "\n",
    "SVR\n",
    "\n",
    "`The best combinations of parameters for **1st grid** search are {'C': 10.0, 'degree': 1, 'epsilon': 0.01, 'gamma': 1.0, 'kernel': 'rbf'} with a score of 0.055 on the validation set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73358f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters are following\n",
    "\n",
    "# etr\n",
    "\n",
    "param_grid = {  'bootstrap': [False], 'max_depth': [20], \n",
    "              'max_features': ['auto'], 'n_estimators': [50],\n",
    "             'min_samples_split': [2], 'min_samples_leaf' : [1]}\n",
    "'''\n",
    "# svr\n",
    "param_grid = {'kernel':['rbf'], 'C': [10.0],\n",
    "                'gamma': [1.0], 'epsilon': [0.01],\n",
    "                'degree': [1]}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d2b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# etr\n",
    "\n",
    "grid_search = GridSearchCV(estimator = etr, param_grid = param_grid, cv = 10,\n",
    "                           scoring = 'neg_mean_absolute_error', return_train_score = True, n_jobs = -1,\n",
    "                          verbose=1000)\n",
    "\n",
    "'''\n",
    "# svr\n",
    "grid_search = GridSearchCV(estimator = svr, param_grid = param_grid, cv = 10,\n",
    "                           scoring = 'neg_mean_absolute_error', return_train_score = True, n_jobs = -1,\n",
    "                          verbose=1000)\n",
    "\n",
    "'''\n",
    "grid_search.fit(X_train, Y_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e227ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best combinations of parameters are %s with a score of %0.3f on the validation set.\"\n",
    "      % (grid_search.best_params_, -grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted Propane selectivity for all the structures\n",
    "\n",
    "Y_pred_val = grid_search.predict(X_val) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = grid_search.predict(X_train)\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "\n",
    "plt.scatter(Y_target_val, Y_pred_val, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.scatter(Y_target_train, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, label='Train set')\n",
    "\n",
    "plt.plot([np.min(Y_target_val),np.max(Y_target_val)], \n",
    "         [np.min(Y_target_val),np.max(Y_target_val)], color='black', linestyle='--')\n",
    "\n",
    "plt.title('Performance of ML model for \\ntrain and validation set', fontdict=fontdict_t, color='black')\n",
    "\n",
    "plt.xlabel('GCMC simulated $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_x)\n",
    "plt.ylabel('ML Predicted $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_y)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "#0.8, 0.3, 0.75, 0.25, 0.7, 0.20, 0.68, 0.15\n",
    "plt.text(0.8, 0.3, str('Train     Val'), weight='bold', horizontalalignment='left', size='medium', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.74, 0.25, str('$\\mathregular{R^2:}$') + '{:.3f}'.format(r2_score(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(r2_score(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.7, 0.20, str('$\\mathregular{MAE: }$') + '{:.3f}'.format(mean_absolute_error(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_absolute_error(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.68, 0.15, str('$\\mathregular{RMSE: }$') + '{:.3f}'.format(mean_squared_error(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_squared_error(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms.eps', format='eps')\n",
    "\n",
    "\n",
    "#print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train_new, Y_pred_train))\n",
    "#print(\"\\nMean absolute error on train set: %0.3f \\n\" %(np.abs(Y_pred_train-Y_train_new)).mean())\n",
    "\n",
    "#print(\"\\nR^2 score on test set: %.3f\\n\" % r2_score(Y_test.iloc[:,1], Y_pred_test))\n",
    "#print(\"\\nMean absolute error on val set: %0.3f\\n\" %(np.abs(Y_pred_test-Y_test.iloc[:,1])).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62160f3",
   "metadata": {},
   "source": [
    "## Shap analysis on train and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b07134",
   "metadata": {},
   "source": [
    "ExtraTreesRegressor\n",
    "\n",
    "`The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} with a score of **0.058** on the validation set.`\n",
    "\n",
    "SVR\n",
    "\n",
    "`The best combinations of parameters for **1st grid** search are {'C': 10.0, 'degree': 1, 'epsilon': 0.01, 'gamma': 1.0, 'kernel': 'rbf'} with a score of 0.055 on the validation set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del etr\n",
    "#del svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ce958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "etr = ExtraTreesRegressor(random_state=RNG_SEED, bootstrap = 'False', max_depth = 20,\n",
    "                            max_features = 'auto', n_estimators = 50, min_samples_split = 2,\n",
    "                            min_samples_leaf = 1)\n",
    "\n",
    "\n",
    "#svr = SVR(C = 10.0, degree = 1, epsilon = 0.01, gamma = 1.0, kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = etr.fit(X_train, Y_target_train)\n",
    "#model = svr.fit(X_train, Y_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dff72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted Propane selectivity for all the structures\n",
    "\n",
    "Y_pred_val = etr.predict(X_val) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = etr.predict(X_train)\n",
    "'''\n",
    "Y_pred_val = svr.predict(X_val) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = svr.predict(X_train)\n",
    "\n",
    "'''\n",
    "#plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "\n",
    "plt.scatter(Y_target_val, Y_pred_val, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.scatter(Y_target_train, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, label='Train set')\n",
    "\n",
    "plt.plot([np.min(Y_target_val),np.max(Y_target_val)], \n",
    "         [np.min(Y_target_val),np.max(Y_target_val)], color='black', linestyle='--')\n",
    "\n",
    "plt.title('Performance of ML model for \\ntrain and validation set', fontdict=fontdict_t, color='black')\n",
    "\n",
    "plt.xlabel('GCMC simulated $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_x)\n",
    "plt.ylabel('ML Predicted $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_y)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "#0.8, 0.3, 0.75, 0.25, 0.7, 0.20, 0.68, 0.15\n",
    "plt.text(0.8, 0.3, str('Train     Val'), weight='bold', horizontalalignment='left', size='medium', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.74, 0.25, str('$\\mathregular{R^2:}$') + '{:.3f}'.format(r2_score(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(r2_score(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.7, 0.20, str('$\\mathregular{MAE: }$') + '{:.3f}'.format(mean_absolute_error(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_absolute_error(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.68, 0.15, str('$\\mathregular{RMSE: }$') + '{:.3f}'.format(mean_squared_error(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_squared_error(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms.eps', format='eps')\n",
    "\n",
    "\n",
    "#print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train_new, Y_pred_train))\n",
    "#print(\"\\nMean absolute error on train set: %0.3f \\n\" %(np.abs(Y_pred_train-Y_train_new)).mean())\n",
    "\n",
    "#print(\"\\nR^2 score on test set: %.3f\\n\" % r2_score(Y_test.iloc[:,1], Y_pred_test))\n",
    "#print(\"\\nMean absolute error on val set: %0.3f\\n\" %(np.abs(Y_pred_test-Y_test.iloc[:,1])).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad05cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8187166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Perform permutation importance\n",
    "results = permutation_importance(model, X_train, Y_target_train, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Get importance\n",
    "importance = results.importances_mean\n",
    "\n",
    "# Summarize feature importance\n",
    "\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.barh([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del shap\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(etr)\n",
    "shap_values = explainer.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f0834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train, feature_names = shap_columns, max_display = 15, show = False)\n",
    "plt.title(\"Feature importance calculated using SHAP for Propane-Uptake (mol/kg) for train set\\n\", fontweight = \"bold\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b3ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229a57e",
   "metadata": {},
   "source": [
    "## SHAP for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer.shap_values(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7564bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_val, feature_names = shap_columns, max_display = 15, show = False)\n",
    "plt.title(\"Feature importance calculated using SHAP for Propane-Uptake (mol/kg) for validation set\\n\", fontweight = \"bold\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf88dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcdaec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "localimportance = 0.0\n",
    "for index, feature_importance in enumerate(grid_search.best_estimator_.feature_importances_):\n",
    "    if feature_importance > localimportance:\n",
    "        localimportance = feature_importance\n",
    "        print(index, feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06b028",
   "metadata": {},
   "source": [
    "## Combining training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the train and validation datasets together\n",
    "X_train_new = np.concatenate((X_train, X_val), axis=0)\n",
    "Y_train_new = pd.concat((Y_target_train, Y_target_val), axis=0)\n",
    "\n",
    "B_X = X_train_new\n",
    "\n",
    "print(X_train_new.shape)\n",
    "print(Y_train_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368650f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del etr\n",
    "#del svr\n",
    "del grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ecd68e",
   "metadata": {},
   "source": [
    "## Retraining model on combined train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "#from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf19f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "etr = ExtraTreesRegressor(random_state=RNG_SEED)\n",
    "#svr = SVR() # random_state=RNG_SEED cannot be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c850425",
   "metadata": {},
   "source": [
    "## Best hyper parameters are: \n",
    "\n",
    "ExtraTreesRegressor\n",
    "\n",
    "`The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 350} with a score of **0.053** on the test set.`\n",
    "\n",
    "SVR\n",
    "\n",
    "`The best combinations of parameters for **2nd grid** search are {'C': 10.0, 'degree': 3, 'epsilon': 0.01, 'gamma': 1.0, 'kernel': 'rbf'} with a score of 0.053 on the test set.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274beec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyper parameters are \n",
    "# etr\n",
    "\n",
    "param_grid = {  'bootstrap': [False], 'max_depth': [20], \n",
    "              'max_features': ['auto'], 'n_estimators': [350],\n",
    "             'min_samples_split': [2], 'min_samples_leaf' : [1]}\n",
    "# svr\n",
    "'''\n",
    "param_grid = {  'C': [10.0], 'degree': [3], \n",
    "              'epsilon': [0.01], 'gamma': [1.0],\n",
    "             'kernel': ['rbf']}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_search = GridSearchCV(estimator = etr, param_grid = param_grid, cv = 10,\n",
    "                           scoring = 'neg_mean_absolute_error', return_train_score = True, n_jobs = -1,\n",
    "                          verbose=1000)\n",
    "'''\n",
    "grid_search = GridSearchCV(estimator = svr, param_grid = param_grid, cv = 10,\n",
    "                           scoring = 'neg_mean_absolute_error', return_train_score = True, n_jobs = -1,\n",
    "                          verbose=1000)\n",
    "'''\n",
    "grid_search.fit(X_train_new, Y_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9c756",
   "metadata": {},
   "source": [
    "## Grid search results\n",
    "\n",
    "Print out the average validation errors and corresponding hyperparameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_result_propane_uptake = pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc37bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cv_result_propane_uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results of all the hyperparameters searched\n",
    "\n",
    "#df_cv_result_propane_uptake = df_cv_result_propane_uptake.astype(float)\n",
    "#df_cv_result_propane_uptake.to_csv('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/grid_search_results/selectivity/1_selectivity_propane_propylene_grid_search_results_excluding_oms.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc098fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_result_propane_uptake.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb26bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(f'\\nGrid search results:\\n\\n {df_cv_result_selectivity}\\n')\n",
    "print('\\n------------------------------------------------------------\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e775cdab",
   "metadata": {},
   "source": [
    "## Printing out the average validation errors and corresponding hyperparameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eab12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test = grid_search.cv_results_['mean_test_score']\n",
    "std_test = grid_search.cv_results_['std_test_score']\n",
    "\n",
    "mean_train = grid_search.cv_results_['mean_train_score']\n",
    "std_train = grid_search.cv_results_['std_train_score']\n",
    "\n",
    "'''\n",
    "for mean, std, params in zip(-means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27075679",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best combinations of parameters are %s with a score of %0.3f on the validation set.\"\n",
    "      % (grid_search.best_params_, -grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted Propane selectivity for all the structures\n",
    "\n",
    "Y_pred_test = grid_search.predict(X_test) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = grid_search.predict(X_train_new)\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "\n",
    "plt.scatter(Y_target_test, Y_pred_test, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.scatter(Y_train_new, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, label='Train set')\n",
    "\n",
    "plt.plot([np.min(Y_target_test),np.max(Y_target_test)], \n",
    "         [np.min(Y_target_test),np.max(Y_target_test)], color='black', linestyle='--')\n",
    "\n",
    "plt.title('Performance of ML model for \\ntrain and test set', fontdict=fontdict_t, color='black')\n",
    "\n",
    "plt.xlabel('GCMC simulated $N_{C_{3}H_{8}}$', fontdict=fontdict_x)\n",
    "plt.ylabel('ML Predicted $N_{C_{3}H_{8}}$', fontdict=fontdict_y)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# 0.8, 0.3, 0.75, 0.25, 0.7, 0.20, 0.68, 0.15\n",
    "\n",
    "plt.text(0.8, 0.3, str('Train     Test'), weight='bold', horizontalalignment='left', size='medium', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.74, 0.25, str('$\\mathregular{R^2:}$') + '{:.3f}'.format(r2_score(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(r2_score(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.7, 0.20, str('$\\mathregular{MAE: }$') + '{:.3f}'.format(mean_absolute_error(Y_train_new, Y_pred_train)) +\n",
    "         str('   ') + '{:.3f}'.format(mean_absolute_error(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.68, 0.15, str('$\\mathregular{RMSE: }$') + '{:.3f}'.format(mean_squared_error(Y_train_new, Y_pred_train)) +\n",
    "         str('   ') + '{:.3f}'.format(mean_squared_error(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms.eps', format='eps')\n",
    "\n",
    "\n",
    "#print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train_new, Y_pred_train))\n",
    "#print(\"\\nMean absolute error on train set: %0.3f \\n\" %(np.abs(Y_pred_train-Y_train_new)).mean())\n",
    "\n",
    "#print(\"\\nR^2 score on test set: %.3f\\n\" % r2_score(Y_test.iloc[:,1], Y_pred_test))\n",
    "#print(\"\\nMean absolute error on val set: %0.3f\\n\" %(np.abs(Y_pred_test-Y_test.iloc[:,1])).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623b569",
   "metadata": {},
   "source": [
    "## Feature importance of rfr model. Note that I think this is option to shap analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788dbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d740b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acfac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c72a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d40c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise ValueError('Testing going on!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b32829",
   "metadata": {},
   "source": [
    "## Shap analysis on train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f23783",
   "metadata": {},
   "source": [
    "ExtraTreesRegressor\n",
    "\n",
    "`The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 350} with a score of **0.053** on the test set.`\n",
    "\n",
    "SVR\n",
    "\n",
    "`The best combinations of parameters for **2nd grid** search are {'C': 10.0, 'degree': 3, 'epsilon': 0.01, 'gamma': 1.0, 'kernel': 'rbf'} with a score of 0.053 on the test set.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del etr\n",
    "#del svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e91cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "etr = ExtraTreesRegressor(random_state=RNG_SEED, bootstrap = 'False', max_depth = 20,\n",
    "                            max_features = 'auto', n_estimators = 100, min_samples_split = 6,\n",
    "                            min_samples_leaf = 1)\n",
    "\n",
    "#svr = SVR(C = 10.0, degree = 3, epsilon = 0.01, gamma = 1.0, kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8aad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = etr.fit(X_train_new, Y_train_new)\n",
    "#model = svr.fit(X_train_new, Y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5960f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf170e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted Propane Uptake for all the structures\n",
    "\n",
    "Y_pred_test = etr.predict(X_test) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = etr.predict(X_train_new)\n",
    "'''\n",
    "\n",
    "Y_pred_test = svr.predict(X_test) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = svr.predict(X_train_new)\n",
    "'''\n",
    "\n",
    "Y_pred_test_model_propane_uptake  = Y_pred_test\n",
    "Y_pred_train_model_propane_uptake = Y_pred_train\n",
    "\n",
    "X_test_model_propane_uptake  = X_test\n",
    "X_train_model_propane_uptake = X_train\n",
    "\n",
    "Y_act_test_model_propane_uptake  = Y_target_test\n",
    "Y_act_train_model_propane_uptake = Y_train_new\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "\n",
    "plt.scatter(Y_target_test, Y_pred_test, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.scatter(Y_train_new, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, label='Train set')\n",
    "\n",
    "plt.plot([np.min(Y_target_test),np.max(Y_target_test)], \n",
    "         [np.min(Y_target_test),np.max(Y_target_test)], color='black', linestyle='--')\n",
    "\n",
    "plt.title('Performance of ML model for \\ntrain and test set', fontdict=fontdict_t, color='black')\n",
    "\n",
    "plt.xlabel('GCMC simulated $N_{C_{3}H_{8}}$', fontdict=fontdict_x)\n",
    "plt.ylabel('ML Predicted $N_{C_{3}H_{8}}$', fontdict=fontdict_y)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "# 0.8, 0.3, 0.75, 0.25, 0.7, 0.20, 0.68, 0.15\n",
    "\n",
    "plt.text(0.8, 0.3, str('Train     Test'), weight='bold', horizontalalignment='left', size='medium', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.74, 0.25, str('$\\mathregular{R^2:}$') + '{:.3f}'.format(r2_score(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(r2_score(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.7, 0.20, str('$\\mathregular{MAE: }$') + '{:.3f}'.format(mean_absolute_error(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_absolute_error(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.68, 0.15, str('$\\mathregular{RMSE: }$') + '{:.3f}'.format(mean_squared_error(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_squared_error(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms.eps', format='eps')\n",
    "\n",
    "\n",
    "#print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train_new, Y_pred_train))\n",
    "#print(\"\\nMean absolute error on train set: %0.3f \\n\" %(np.abs(Y_pred_train-Y_train_new)).mean())\n",
    "\n",
    "#print(\"\\nR^2 score on test set: %.3f\\n\" % r2_score(Y_test.iloc[:,1], Y_pred_test))\n",
    "#print(\"\\nMean absolute error on val set: %0.3f\\n\" %(np.abs(Y_pred_test-Y_test.iloc[:,1])).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafaf2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_act_train_model_propane_uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea05caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise ValueError('Testing going on!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219abf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Perform permutation importance\n",
    "results = permutation_importance(model, X_train, Y_target_train, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Get importance\n",
    "importance = results.importances_mean\n",
    "\n",
    "# Summarize feature importance\n",
    "\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.barh([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "del shap\n",
    "\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(etr)\n",
    "shap_values = explainer.shap_values(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54922a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train_new, feature_names = shap_columns, max_display = 15, show = False)\n",
    "plt.title(\"Feature importance calculated using SHAP for Propane Uptake (mol/kg) for train + validation set\\n\", fontweight = \"bold\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0046877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62159b50",
   "metadata": {},
   "source": [
    "## SHAP for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a26870",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a06139",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, feature_names = shap_columns, max_display = 15, show = False)\n",
    "plt.title(\"Feature importance calculated using SHAP for Propane Uptake (mol/kg) for test set\\n\", fontweight = \"bold\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6471a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d9337",
   "metadata": {},
   "outputs": [],
   "source": [
    "localimportance = 0.0\n",
    "for index, feature_importance in enumerate(grid_search.best_estimator_.feature_importances_):\n",
    "    if feature_importance > localimportance:\n",
    "        localimportance = feature_importance\n",
    "        print(index, feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11215295",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_propane_uptake = grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3102ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "del etr\n",
    "#del svr\n",
    "del param_grid\n",
    "del grid_search\n",
    "del GridSearchCV\n",
    "#del ticks\n",
    "#del mean_val\n",
    "del mean_train\n",
    "#del std_val\n",
    "#del std_train\n",
    "del Y_pred_val\n",
    "del Y_pred_train\n",
    "del localimportance\n",
    "del explainer\n",
    "del index\n",
    "del feature_importance\n",
    "del df_cv_result_propane_uptake\n",
    "del shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59184e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise ValueError('Testing going on!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe8e28",
   "metadata": {},
   "source": [
    "## RFE (Recursive Feature Elimination) for propylene uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7167cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment when model has to be trained on crude data\n",
    "\n",
    "#X_train = X_train_crude\n",
    "#X_val   = X_val_crude\n",
    "#X_test  = X_test_crude\n",
    "\n",
    "## Uncomment when model has to be trained on scaled data\n",
    "\n",
    "#X_train = X_train_scaled\n",
    "#X_val   = X_val_scaled\n",
    "#X_test  = X_test_scaled\n",
    "\n",
    "## Uncomment when model has to be trained on normalised data\n",
    "#X_train = X_train_norm\n",
    "#X_val   = X_val_norm\n",
    "#X_test  = X_test_norm\n",
    "\n",
    "## Uncomment when model has to be trained on scaled_normalised  data\n",
    "X_train  = X_train_scaled_norm\n",
    "X_val    = X_val_scaled_norm\n",
    "X_test   = X_test_scaled_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4119f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Y is neigther scaled nor normalized\n",
    "\n",
    "# If index is 0 then, propane / ethane uptake (mol/kg)  \n",
    "# If index is 1 then, selectivity\n",
    "# If index is 2 then, TSN\n",
    "# If index is 3 then, propylene / ethylene uptake (mol/kg)\n",
    "\n",
    "Y_target_train = Y_train_crude.iloc[:,3]\n",
    "Y_target_test  = Y_test_crude.iloc[:,3]\n",
    "Y_target_val   = Y_val_crude.iloc[:,3]\n",
    "\n",
    "temp_train = Y_train_crude.iloc[:,1]\n",
    "temp_test  = Y_test_crude.iloc[:,1]\n",
    "temp_val   = Y_val_crude.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebcbbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a06c3",
   "metadata": {},
   "source": [
    "**(Propane + propylene_uptake + atomic + excluding + scaled + normalized)**\n",
    "\n",
    "**For RFE using GBR best features obtained were**\n",
    "`Index(['GSA(2)','du(7)','H(11)','Zn(15)','LCD(1)','Density(5)','tdu(6)','AVF(3)','S(22)','GPV'(4), 'N(21), 'Br(25)', 'Pb(16)'], dtype='object') ` (13 features). The best model found was **ExtraTreesRegressor** and second best model is **SVR** so we did hyper parameter optimisation and we got best hyperparameters as: \n",
    "\n",
    "X_train = X_train[:,[2,7,11,15,1,5,6,3,22,4,21,25,16]]\n",
    "X_test  = X_test[:,[2,7,11,15,1,5,6,3,22,4,21,25,16]]\n",
    "X_val   = X_val[:,[2,7,11,15,1,5,6,3,22,4,21,25,16]]\n",
    "\n",
    "shap_columns = X_crude.columns[[2,7,11,15,1,5,6,3,22,4,21,25,16]]\n",
    "\n",
    "**1. GBR(RFE) + ExtraTreesRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 250} with a score of **0.249** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 30, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 300} with a score of **0.226** on the validation set.\n",
    "\n",
    "\n",
    "**2. GBR(RFE) + SVR(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'C': 100.0, 'degree': 1, 'epsilon': 0.1, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.231** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'C': 10.0, 'degree': 3, 'epsilon': 0.1, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.230** on the test set.\n",
    "\n",
    "\n",
    "**For RFE using RFR best features obtained were**\n",
    "`Index(['PLD(0)','LCD(1)','GSA(2)','du(7)','H(11)','Zn(15)','C'(19),'O to M ratio (9)','AVF(3)','F(25)','Co(13)','Pb(16)'], dtype='object') ` (11 features)\n",
    "\n",
    "X_train = X_train[:,[0,1,2,7,11,15,19,9,3,25,13,16]]\n",
    "X_test  = X_test[:,[0,1,2,7,11,15,19,9,3,25,13,16]]\n",
    "X_val   = X_val[:,[0,1,2,7,11,15,19,9,3,25,13,16]]\n",
    "\n",
    "shap_columns = X_crude.columns[[0,1,2,7,11,15,19,9,3,25,13,16]]\n",
    "\n",
    "**3. RFR(RFE) + ExtraTreesRegressor(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200} with a score of **0.243** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 100} with a score of **0.222** on the test set.\n",
    "\n",
    "**4. RFR(RFE) + SVR(best_model)**\n",
    "\n",
    "The best combinations of parameters for **1st grid** search are {'C': 100.0, 'degree': 1, 'epsilon': 0.1, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.240** on the validation set.\n",
    "\n",
    "The best combinations of parameters for **2nd grid** search are {'C': 100.0, 'degree': 3, 'epsilon': 0.01, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.228** on the test set.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e009889",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_crude.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e8d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Propane + propylene_uptake + atomic + excluding + scaled + normalized) + GBR\n",
    "\n",
    "X_train = X_train[:,[2,7,11,15,1,5,6,3,22,4,21,25,16]]\n",
    "X_test  = X_test[:,[2,7,11,15,1,5,6,3,22,4,21,25,16]]\n",
    "X_val   = X_val[:,[2,7,11,15,1,5,6,3,22,4,21,25,16]]\n",
    "\n",
    "shap_columns = X_crude.columns[[2,7,11,15,1,5,6,3,22,4,21,25,16]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7d2f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise ValueError('Testing going on!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48080633",
   "metadata": {},
   "source": [
    "## Grid search for Propylene uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b2498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "#from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e304a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "etr = ExtraTreesRegressor(random_state=RNG_SEED)\n",
    "#svr = SVR() # random_state=RNG_SEED cannot be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e955c4",
   "metadata": {},
   "source": [
    "## Best hyper parameters are:\n",
    "\n",
    "ExtraTreesRegressor\n",
    "\n",
    "`The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 250} with a score of **0.249** on the validation set.`\n",
    "\n",
    "SVR\n",
    "\n",
    "`The best combinations of parameters for **1st grid** search are {'C': 100.0, 'degree': 1, 'epsilon': 0.1, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.231** on the validation set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d8d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters are following\n",
    "\n",
    "# etr\n",
    "\n",
    "param_grid = {  'bootstrap': [False], 'max_depth': [20], \n",
    "              'max_features': ['auto'], 'n_estimators': [250],\n",
    "             'min_samples_split': [4], 'min_samples_leaf' : [1]}\n",
    "'''\n",
    "# svr\n",
    "param_grid = {'kernel':['rbf'], 'C': [100.0],\n",
    "                'gamma': [1.0], 'epsilon': [0.1],\n",
    "                'degree': [1]}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742afa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# etr\n",
    "\n",
    "grid_search = GridSearchCV(estimator = etr, param_grid = param_grid, cv = 10,\n",
    "                           scoring = 'neg_mean_absolute_error', return_train_score = True, n_jobs = -1,\n",
    "                          verbose=1000)\n",
    "\n",
    "'''\n",
    "# svr\n",
    "grid_search = GridSearchCV(estimator = svr, param_grid = param_grid, cv = 10,\n",
    "                           scoring = 'neg_mean_absolute_error', return_train_score = True, n_jobs = -1,\n",
    "                          verbose=1000)\n",
    "'''\n",
    "\n",
    "grid_search.fit(X_train, Y_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c189bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best combinations of parameters are %s with a score of %0.3f on the validation set.\"\n",
    "      % (grid_search.best_params_, -grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc920c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted Propane selectivity for all the structures\n",
    "\n",
    "Y_pred_val = grid_search.predict(X_val) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = grid_search.predict(X_train)\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "\n",
    "plt.scatter(Y_target_val, Y_pred_val, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.scatter(Y_target_train, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, label='Train set')\n",
    "\n",
    "plt.plot([np.min(Y_target_val),np.max(Y_target_val)], \n",
    "         [np.min(Y_target_val),np.max(Y_target_val)], color='black', linestyle='--')\n",
    "\n",
    "plt.title('Performance of ML model for \\ntrain and validation set', fontdict=fontdict_t, color='black')\n",
    "\n",
    "plt.xlabel('GCMC simulated $N_{C_{3}H_{6}}$', fontdict=fontdict_x)\n",
    "plt.ylabel('ML Predicted $N_{C_{3}H_{6}}$', fontdict=fontdict_y)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "#3.0, 1.3, 2.7, 1.1, 2.5, 0.9, 2.41, 0.7\n",
    "plt.text(3.0, 1.3, str('Train     Val'), weight='bold', horizontalalignment='left', size='medium', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.74, 0.25, str('$\\mathregular{R^2:}$') + '{:.3f}'.format(r2_score(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(r2_score(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(2.7, 1.1, str('$\\mathregular{MAE: }$') + '{:.3f}'.format(mean_absolute_error(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_absolute_error(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(2.41, 0.7, str('$\\mathregular{RMSE: }$') + '{:.3f}'.format(mean_squared_error(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_squared_error(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms.eps', format='eps')\n",
    "\n",
    "\n",
    "#print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train_new, Y_pred_train))\n",
    "#print(\"\\nMean absolute error on train set: %0.3f \\n\" %(np.abs(Y_pred_train-Y_train_new)).mean())\n",
    "\n",
    "#print(\"\\nR^2 score on test set: %.3f\\n\" % r2_score(Y_test.iloc[:,1], Y_pred_test))\n",
    "#print(\"\\nMean absolute error on val set: %0.3f\\n\" %(np.abs(Y_pred_test-Y_test.iloc[:,1])).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c9237",
   "metadata": {},
   "source": [
    "## Shap analysis on train and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf09b18",
   "metadata": {},
   "source": [
    "ExtraTreesRegressor\n",
    "\n",
    "`The best combinations of parameters for **1st grid** search are {'bootstrap': False, 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 250} with a score of **0.249** on the validation set.`\n",
    "\n",
    "SVR\n",
    "\n",
    "`The best combinations of parameters for **1st grid** search are {'C': 100.0, 'degree': 1, 'epsilon': 0.1, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.231** on the validation set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69623d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "del etr\n",
    "#del svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd863025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "etr = ExtraTreesRegressor(random_state=RNG_SEED, bootstrap = 'False', max_depth = 20,\n",
    "                            max_features = 'auto', n_estimators = 250, min_samples_split = 4,\n",
    "                            min_samples_leaf = 1)\n",
    "\n",
    "\n",
    "#svr = SVR(C = 100.0, degree = 1, epsilon = 0.1, gamma = 1.0, kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff102bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = etr.fit(X_train, Y_target_train)\n",
    "#model = svr.fit(X_train, Y_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e01c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted Propane selectivity for all the structures\n",
    "\n",
    "Y_pred_val = etr.predict(X_val) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = etr.predict(X_train)\n",
    "'''\n",
    "Y_pred_val = svr.predict(X_val) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = svr.predict(X_train)\n",
    "'''\n",
    "#plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "\n",
    "plt.scatter(Y_target_val, Y_pred_val, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.scatter(Y_target_train, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, label='Train set')\n",
    "\n",
    "plt.plot([np.min(Y_target_val),np.max(Y_target_val)], \n",
    "         [np.min(Y_target_val),np.max(Y_target_val)], color='black', linestyle='--')\n",
    "\n",
    "plt.title('Performance of ML model for \\ntrain and validation set', fontdict=fontdict_t, color='black')\n",
    "\n",
    "plt.xlabel('GCMC simulated $N_{C_{3}H_{6}}$', fontdict=fontdict_x)\n",
    "plt.ylabel('ML Predicted $N_{C_{3}H_{6}}$', fontdict=fontdict_y)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "#3.0, 1.3, 2.7, 1.1, 2.5, 0.9, 2.41, 0.7\n",
    "plt.text(3.0, 1.3, str('Train     Val'), weight='bold', horizontalalignment='left', size='medium', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.text(0.74, 0.25, str('$\\mathregular{R^2:}$') + '{:.3f}'.format(r2_score(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(r2_score(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(2.5, 0.9, str('$\\mathregular{MAE: }$') + '{:.3f}'.format(mean_absolute_error(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_absolute_error(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(2.41, 0.7, str('$\\mathregular{RMSE: }$') + '{:.3f}'.format(mean_squared_error(Y_target_train, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_squared_error(Y_target_val, Y_pred_val)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms.eps', format='eps')\n",
    "\n",
    "\n",
    "#print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train_new, Y_pred_train))\n",
    "#print(\"\\nMean absolute error on train set: %0.3f \\n\" %(np.abs(Y_pred_train-Y_train_new)).mean())\n",
    "\n",
    "#print(\"\\nR^2 score on test set: %.3f\\n\" % r2_score(Y_test.iloc[:,1], Y_pred_test))\n",
    "#print(\"\\nMean absolute error on val set: %0.3f\\n\" %(np.abs(Y_pred_test-Y_test.iloc[:,1])).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257da43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Perform permutation importance\n",
    "results = permutation_importance(model, X_train, Y_target_train, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Get importance\n",
    "importance = results.importances_mean\n",
    "\n",
    "# Summarize feature importance\n",
    "\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.barh([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ec408",
   "metadata": {},
   "outputs": [],
   "source": [
    "del shap\n",
    "\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(etr)\n",
    "shap_values = explainer.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train, feature_names = shap_columns, max_display = 15, show = False)\n",
    "plt.title(\"Feature importance calculated using SHAP for propylene-Uptake (mol/kg) for train set\\n\", fontweight = \"bold\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af4b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57b99e",
   "metadata": {},
   "source": [
    "## SHAP for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b04c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer.shap_values(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_val, feature_names = shap_columns, max_display = 15, show = False)\n",
    "plt.title(\"Feature importance calculated using SHAP for propylene-Uptake (mol/kg) for validation set\\n\", fontweight = \"bold\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a03cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44992fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "localimportance = 0.0\n",
    "for index, feature_importance in enumerate(grid_search.best_estimator_.feature_importances_):\n",
    "    if feature_importance > localimportance:\n",
    "        localimportance = feature_importance\n",
    "        print(index, feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dade1c",
   "metadata": {},
   "source": [
    "## Combining training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the train and validation datasets together\n",
    "X_train_new = np.concatenate((X_train, X_val), axis=0)\n",
    "Y_train_new = pd.concat((Y_target_train, Y_target_val), axis=0)\n",
    "\n",
    "temp_train_val_comb = pd.concat((temp_train, temp_val), axis=0)\n",
    "\n",
    "C_X = X_train_new\n",
    "\n",
    "print(X_train_new.shape)\n",
    "print(Y_train_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43328db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d88f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train_val_comb # selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del etr\n",
    "#del svr\n",
    "del grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab97777",
   "metadata": {},
   "source": [
    "## Retraining model on combined train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0df7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "#from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5648937",
   "metadata": {},
   "outputs": [],
   "source": [
    "etr = ExtraTreesRegressor(random_state=RNG_SEED)\n",
    "#svr = SVR() # random_state=RNG_SEED cannot be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978cf7bf",
   "metadata": {},
   "source": [
    "## Best hyper parameters are: \n",
    "\n",
    "ExtraTreesRegressor\n",
    "\n",
    "`The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 30, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 300} with a score of **0.226** on the validation set.`\n",
    "\n",
    "SVR\n",
    "\n",
    "`The best combinations of parameters for **2nd grid** search are {'C': 10.0, 'degree': 3, 'epsilon': 0.1, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.230** on the test set.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyper parameters are \n",
    "# etr\n",
    "\n",
    "param_grid = {  'bootstrap': [False], 'max_depth': [30], \n",
    "              'max_features': ['auto'], 'n_estimators': [300],\n",
    "             'min_samples_split': [6], 'min_samples_leaf' : [1]}\n",
    "# svr\n",
    "'''\n",
    "param_grid = {  'C': [10.0], 'degree': [3], \n",
    "              'epsilon': [0.1], 'gamma': [1.0],\n",
    "             'kernel': ['rbf']}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b1e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_search = GridSearchCV(estimator = etr, param_grid = param_grid, cv = 10,\n",
    "                           scoring = 'neg_mean_absolute_error', return_train_score = True, n_jobs = -1,\n",
    "                          verbose=1000)\n",
    "'''\n",
    "grid_search = GridSearchCV(estimator = svr, param_grid = param_grid, cv = 10,\n",
    "                           scoring = 'neg_mean_absolute_error', return_train_score = True, n_jobs = -1,\n",
    "                          verbose=1000)\n",
    "'''\n",
    "grid_search.fit(X_train_new, Y_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e0894",
   "metadata": {},
   "source": [
    "## Grid search results\n",
    "\n",
    "Print out the average validation errors and corresponding hyperparameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_result_propylene_uptake = pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a97ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cv_result_propylene_uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1876b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results of all the hyperparameters searched\n",
    "\n",
    "#df_cv_result_propylene_uptake = df_cv_result_propane_uptake.astype(float)\n",
    "#df_cv_result_propylene_uptake.to_csv('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/grid_search_results/selectivity/1_selectivity_propane_propylene_grid_search_results_excluding_oms.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_result_propylene_uptake.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d81135",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(f'\\nGrid search results:\\n\\n {df_cv_result_selectivity}\\n')\n",
    "print('\\n------------------------------------------------------------\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4aad39",
   "metadata": {},
   "source": [
    "## Printing out the average validation errors and corresponding hyperparameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a91d3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test = grid_search.cv_results_['mean_test_score']\n",
    "std_test = grid_search.cv_results_['std_test_score']\n",
    "\n",
    "mean_train = grid_search.cv_results_['mean_train_score']\n",
    "std_train = grid_search.cv_results_['std_train_score']\n",
    "\n",
    "'''\n",
    "for mean, std, params in zip(-means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab996b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best combinations of parameters are %s with a score of %0.3f on the validation set.\"\n",
    "      % (grid_search.best_params_, -grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cca823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted Propane selectivity for all the structures\n",
    "\n",
    "Y_pred_test = grid_search.predict(X_test) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = grid_search.predict(X_train_new)\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "\n",
    "plt.scatter(Y_target_test, Y_pred_test, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.scatter(Y_train_new, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, label='Train set')\n",
    "\n",
    "plt.plot([np.min(Y_target_test),np.max(Y_target_test)], \n",
    "         [np.min(Y_target_test),np.max(Y_target_test)], color='black', linestyle='--')\n",
    "\n",
    "plt.title('Performance of ML model for \\ntrain and test set', fontdict=fontdict_t, color='black')\n",
    "\n",
    "plt.xlabel('GCMC simulated $N_{C_{3}H_{6}}$', fontdict=fontdict_x)\n",
    "plt.ylabel('ML Predicted $N_{C_{3}H_{6}}$', fontdict=fontdict_y)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "#3.0, 1.3, 2.7, 1.1, 2.5, 0.9, 2.41, 0.7\n",
    "\n",
    "plt.text(3.0, 1.3, str('Train     Test'), weight='bold', horizontalalignment='left', size='medium', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.text(2.7, 1.1, str('$\\mathregular{R^2:}$') + '{:.3f}'.format(r2_score(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(r2_score(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(2.5, 0.9, str('$\\mathregular{MAE: }$') + '{:.3f}'.format(mean_absolute_error(Y_train_new, Y_pred_train)) +\n",
    "         str('   ') + '{:.3f}'.format(mean_absolute_error(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(2.41, 0.7, str('$\\mathregular{RMSE: }$') + '{:.3f}'.format(mean_squared_error(Y_train_new, Y_pred_train)) +\n",
    "         str('   ') + '{:.3f}'.format(mean_squared_error(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms.eps', format='eps')\n",
    "\n",
    "\n",
    "#print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train_new, Y_pred_train))\n",
    "#print(\"\\nMean absolute error on train set: %0.3f \\n\" %(np.abs(Y_pred_train-Y_train_new)).mean())\n",
    "\n",
    "#print(\"\\nR^2 score on test set: %.3f\\n\" % r2_score(Y_test.iloc[:,1], Y_pred_test))\n",
    "#print(\"\\nMean absolute error on val set: %0.3f\\n\" %(np.abs(Y_pred_test-Y_test.iloc[:,1])).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ab5e4",
   "metadata": {},
   "source": [
    "## Feature importance of rfr model. Note that I think this is option to shap analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49221692",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise ValueError('Testing going on!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc403c",
   "metadata": {},
   "source": [
    "## Shap analysis on train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286cdba",
   "metadata": {},
   "source": [
    "ExtraTreesRegressor\n",
    "\n",
    "`The best combinations of parameters for **2nd grid** search are {'bootstrap': False, 'max_depth': 30, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 300} with a score of **0.226** on the validation set.`\n",
    "\n",
    "SVR\n",
    "\n",
    "`The best combinations of parameters for **2nd grid** search are {'C': 10.0, 'degree': 3, 'epsilon': 0.1, 'gamma': 1.0, 'kernel': 'rbf'} with a score of **0.230** on the test set.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del etr\n",
    "#del svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424589e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "etr = ExtraTreesRegressor(random_state=RNG_SEED, bootstrap = 'False', max_depth = 30,\n",
    "                            max_features = 'auto', n_estimators = 300, min_samples_split = 6,\n",
    "                            min_samples_leaf = 1)\n",
    "\n",
    "#svr = SVR(C = 10.0, degree = 3, epsilon = 0.1, gamma = 1.0, kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = etr.fit(X_train_new, Y_train_new)\n",
    "#model = svr.fit(X_train_new, Y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d512693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted Propane Uptake for all the structures\n",
    "\n",
    "Y_pred_test = etr.predict(X_test) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = etr.predict(X_train_new)\n",
    "\n",
    "'''\n",
    "Y_pred_test = svr.predict(X_test) # scikit-learn automatically takes the best combination\n",
    "                                     # of hyperparameters from grid search\n",
    "\n",
    "Y_pred_train = svr.predict(X_train_new)\n",
    "'''\n",
    "\n",
    "Y_pred_test_model_propylene_uptake  = Y_pred_test\n",
    "Y_pred_train_model_propylene_uptake = Y_pred_train\n",
    "\n",
    "X_test_model_propylene_uptake  = X_test\n",
    "X_train_model_propylene_uptake = X_train\n",
    "\n",
    "Y_act_test_model_propylene_uptake  = Y_target_test\n",
    "Y_act_train_model_propylene_uptake = Y_train_new\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "\n",
    "plt.scatter(Y_target_test, Y_pred_test, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.scatter(Y_train_new, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, label='Train set')\n",
    "\n",
    "plt.plot([np.min(Y_target_test),np.max(Y_target_test)], \n",
    "         [np.min(Y_target_test),np.max(Y_target_test)], color='black', linestyle='--')\n",
    "\n",
    "plt.title('Performance of ML model for \\ntrain and test set', fontdict=fontdict_t, color='black')\n",
    "\n",
    "plt.xlabel('GCMC simulated $N_{C_{3}H_{6}}$', fontdict=fontdict_x)\n",
    "plt.ylabel('ML Predicted $N_{C_{3}H_{6}}$', fontdict=fontdict_y)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "#3.0, 1.3, 2.7, 1.1, 2.5, 0.9, 2.41, 0.7\n",
    "\n",
    "plt.text(3.0, 1.3, str('Train     Test'), weight='bold', horizontalalignment='left', size='medium', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.text(2.7, 1.1, str('$\\mathregular{R^2:}$') + '{:.3f}'.format(r2_score(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(r2_score(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(2.5, 0.9, str('$\\mathregular{MAE: }$') + '{:.3f}'.format(mean_absolute_error(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_absolute_error(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(2.41, 0.7, str('$\\mathregular{RMSE: }$') + '{:.3f}'.format(mean_squared_error(Y_train_new, Y_pred_train)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_squared_error(Y_target_test, Y_pred_test)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms.eps', format='eps')\n",
    "\n",
    "\n",
    "#print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train_new, Y_pred_train))\n",
    "#print(\"\\nMean absolute error on train set: %0.3f \\n\" %(np.abs(Y_pred_train-Y_train_new)).mean())\n",
    "\n",
    "#print(\"\\nR^2 score on test set: %.3f\\n\" % r2_score(Y_test.iloc[:,1], Y_pred_test))\n",
    "#print(\"\\nMean absolute error on val set: %0.3f\\n\" %(np.abs(Y_pred_test-Y_test.iloc[:,1])).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68446641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise ValueError('Testing going on!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Perform permutation importance\n",
    "results = permutation_importance(model, X_train, Y_target_train, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Get importance\n",
    "importance = results.importances_mean\n",
    "\n",
    "# Summarize feature importance\n",
    "\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.barh([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del shap\n",
    "\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(etr)\n",
    "shap_values = explainer.shap_values(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b88962",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train_new, feature_names = shap_columns, max_display = 15, show = False)\n",
    "plt.title(\"Feature importance calculated using SHAP for Propylene uptake (mol/kg) for train+validation set\\n\", fontweight = \"bold\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b6438",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68e3dec",
   "metadata": {},
   "source": [
    "## SHAP for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a8561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, feature_names = shap_columns, max_display = 15, show = False)\n",
    "plt.title(\"Feature importance calculated using SHAP for Propylene uptake (mol/kg) for test set set\\n\", fontweight = \"bold\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/shap/1_selectivity_propane_propylene_shap_excluding_oms.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcdec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "localimportance = 0.0\n",
    "for index, feature_importance in enumerate(grid_search.best_estimator_.feature_importances_):\n",
    "    if feature_importance > localimportance:\n",
    "        localimportance = feature_importance\n",
    "        print(index, feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_propylene_uptake = grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3748c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del etr\n",
    "#del svr\n",
    "del param_grid\n",
    "del grid_search\n",
    "del GridSearchCV\n",
    "#del ticks\n",
    "#del mean_val\n",
    "del mean_train\n",
    "#del std_val\n",
    "#del std_train\n",
    "del Y_pred_val\n",
    "del Y_pred_train\n",
    "del localimportance\n",
    "del explainer\n",
    "del index\n",
    "del feature_importance\n",
    "del df_cv_result_propylene_uptake\n",
    "del shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ffb890",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87235c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62450e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066df3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train_new_model_selectivity = (Y_pred_train_model_propane_uptake / Y_pred_train_model_propylene_uptake) * (0.85 / 0.15)\n",
    "Y_pred_test_new_model_selectivity = (Y_pred_test_model_propane_uptake / Y_pred_test_model_propylene_uptake) * (0.85 / 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14011ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3164d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_act_train_model_propane_uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41349650",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train_model_propane_uptake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_act_train_model_propylene_uptake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train_model_propylene_uptake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_act_train_model_selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec43f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train_model_selectivity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f660581",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test_model_selectivity\n",
    "Y_pred_train_model_selectivity\n",
    "\n",
    "Y_act_test_model_selectivity\n",
    "Y_act_train_model_selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e00a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test_model_propane_uptake \n",
    "Y_pred_train_model_propane_uptake \n",
    "\n",
    "Y_act_test_model_propane_uptake \n",
    "Y_act_train_model_propane_uptake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51bd89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test_model_propylene_uptake \n",
    "Y_pred_train_model_propylene_uptake \n",
    "\n",
    "Y_act_test_model_propylene_uptake \n",
    "Y_act_train_model_propylene_uptake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187999b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted Propane selectivity for all the structures\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "\n",
    "plt.scatter(Y_act_test_model_selectivity, Y_pred_test_new_model_selectivity, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "            label='Test set')\n",
    "\n",
    "plt.scatter(Y_act_train_model_selectivity, Y_pred_train_new_model_selectivity, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, label='Train set')\n",
    "\n",
    "plt.plot([np.min(Y_act_test_model_selectivity),np.max(Y_act_test_model_selectivity)], \n",
    "         [np.min(Y_act_test_model_selectivity),np.max(Y_act_test_model_selectivity)], color='black', linestyle='--')\n",
    "\n",
    "plt.title('Performance of ML model for \\ntrain and test set', fontdict=fontdict_t, color='black')\n",
    "\n",
    "plt.xlabel('GCMC simulated $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_x)\n",
    "plt.ylabel('ML Predicted $S_{C_{3}H_{8}/C_{3}H_{6}}$', fontdict=fontdict_y)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "ticks = [1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0,2.1]\n",
    "plt.axes().set_xticks(ticks)\n",
    "plt.axes().set_yticks(ticks)\n",
    "\n",
    "#[1.75, 1.32, 1.7, 1.27, 1.665, 1.24, 1.65, 1.21]\n",
    "\n",
    "plt.text(1.75, 1.32, str('Train     Test'), weight='bold', horizontalalignment='left', size='medium', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.7, 1.27, str('$\\mathregular{R^2:}$') + '{:.3f}'.format(r2_score(Y_act_train_model_selectivity, Y_pred_train_new_model_selectivity)) + \n",
    "         str('   ') + '{:.3f}'.format(r2_score(Y_act_test_model_selectivity, Y_pred_test_new_model_selectivity)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.665, 1.23, str('$\\mathregular{MAE: }$') + '{:.3f}'.format(mean_absolute_error(Y_act_train_model_selectivity, Y_pred_train_new_model_selectivity)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_absolute_error(Y_act_test_model_selectivity, Y_pred_test_new_model_selectivity)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.text(1.645, 1.19, str('$\\mathregular{RMSE: }$') + '{:.3f}'.format(mean_squared_error(Y_act_train_model_selectivity, Y_pred_train_new_model_selectivity)) + \n",
    "         str('   ') + '{:.3f}'.format(mean_squared_error(Y_act_test_model_selectivity, Y_pred_test_new_model_selectivity)), weight='bold', \n",
    "         horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms_excluding_oms.png', dpi=300)\n",
    "#plt.savefig('/home/varad/varad/literature/24_sauradeep/workspace/ML/output/2_Gross1_Atomic/performance/1_a_propane_propylene_performcance_selectivity_excluding_oms.eps', format='eps')\n",
    "\n",
    "\n",
    "#print(\"\\nR^2 score on train set: %.3f\\n\" % r2_score(Y_train_new, Y_pred_train))\n",
    "#print(\"\\nMean absolute error on train set: %0.3f \\n\" %(np.abs(Y_pred_train-Y_train_new)).mean())\n",
    "\n",
    "#print(\"\\nR^2 score on test set: %.3f\\n\" % r2_score(Y_test.iloc[:,1], Y_pred_test))\n",
    "#print(\"\\nMean absolute error on val set: %0.3f\\n\" %(np.abs(Y_pred_test-Y_test.iloc[:,1])).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e6f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = pd.DataFrame(Y_pred_train_new_model_selectivity)\n",
    "predict.to_csv('2_nd_model_selectivity_train_predict.csv', index=False)\n",
    "\n",
    "predict = pd.DataFrame(Y_act_train_model_selectivity)\n",
    "predict.to_csv('2_nd_model_selectivity_train_actual.csv', index=False)\n",
    "\n",
    "predict = pd.DataFrame(Y_pred_test_new_model_selectivity)\n",
    "predict.to_csv('2_nd_model_selectivity_test_predict.csv', index=False)\n",
    "\n",
    "predict = pd.DataFrame(Y_act_test_model_selectivity)\n",
    "predict.to_csv('2_nd_model_selectivity_test_actual.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f30a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "295.844px",
    "left": "612.667px",
    "right": "20px",
    "top": "84px",
    "width": "544px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
