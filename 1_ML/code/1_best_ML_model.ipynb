{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f1202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "#mpl.rc('figure', max_open_warning = 0)\n",
    "#%matplotlib inline\n",
    "#%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class style:\n",
    "   BOLD = '\\033[1m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd() # Getting current directory\n",
    "descriptor_in_path = os.path.join(PATH, '../input/descriptor.csv')\n",
    "\n",
    "df_descriptor = pd.read_csv(descriptor_in_path)\n",
    "\n",
    "print(f'Descriptor input DataFrame shape:\\n\\n {df_descriptor.shape}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "print(f'\\nDescriptor input data columns:\\n\\n {df_descriptor.columns}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "print(f'\\nDescriptor input dataframe head:\\n\\n {df_descriptor.head()}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "del descriptor_in_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f2bea",
   "metadata": {},
   "source": [
    "## Renaming descriptor columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a9e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {'name': 'mof', 'Di': 'LCD', 'Df': 'PLD', 'ASA(m2/gram)_1.9': 'GSA', \n",
    "               'AV_Volume_fraction_1.9': 'AVF', 'AV(cm3/gram)_1.9': 'GPV', 'density(gram_cm3)': 'Density'}\n",
    "\n",
    "df_descriptor = df_descriptor.rename(columns=rename_dict)\n",
    "\n",
    "print(f'\\nCurated descriptor columns:\\n\\n {df_descriptor.columns}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "print(df_descriptor.dtypes) # Prints the datatype of each column in dataframe\n",
    "del rename_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5abbda",
   "metadata": {},
   "source": [
    "## Curating descriptor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3506d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_descriptor_gross1_atomic = df_descriptor\n",
    "\n",
    "# Selecting materials with PLD > 3.8 A\n",
    "\n",
    "df_descriptor_gross1_atomic = df_descriptor_gross1_atomic[(df_descriptor_gross1_atomic['PLD'] > 3.8)]\n",
    "\n",
    "# Selecting materials with non-zero void fraction\n",
    "\n",
    "df_descriptor_gross1_atomic = df_descriptor_gross1_atomic[(df_descriptor_gross1_atomic['AVF'] > 0.0)]\n",
    "\n",
    "descriptor_mof_name = df_descriptor_gross1_atomic['mof'].astype(str)\n",
    "\n",
    "PATH = os.getcwd() # Getting current directory\n",
    "curated_mof_name = os.path.join(PATH, '../output/curated-mof.csv')\n",
    "descriptor_mof_name.to_csv(curated_mof_name, index=False)\n",
    "\n",
    "columns = ['PLD', 'LCD', 'GSA', 'AVF', 'GPV', 'Density', 'total_degree_unsaturation', 'degree_unsaturation', \n",
    "           'metallic_percentage', 'O_to_Metal_ration', 'N_to_O_ratio', 'H' ,'Ni', 'Co', 'Cu', 'Zn', 'Pb', 'Mn',\n",
    "           'Cd', 'C', 'O', 'N', 'S', 'Cl', 'Br', 'F', 'I']\n",
    "\n",
    "shap_columns = columns\n",
    "\n",
    "df_descriptor_gross1_atomic = df_descriptor_gross1_atomic[columns].astype(float)\n",
    "curated_mof_prop = os.path.join(PATH, '../output/curated-mof-prop.csv')\n",
    "\n",
    "df_descriptor_gross1_atomic.to_csv(curated_mof_prop, index=False)\n",
    "\n",
    "print(f'\\nCurated gross1_atomic descriptor data:\\n\\n {df_descriptor_gross1_atomic}\\n')\n",
    "print('\\n------------------------------------------------------------\\n')\n",
    "\n",
    "print(f'\\nData type of each column. Note that it should be float\\n\\n {df_descriptor_gross1_atomic.dtypes}\\n')\n",
    "print('\\n------------------------------------------------------------\\n')\n",
    "\n",
    "del df_descriptor\n",
    "del columns\n",
    "del descriptor_mof_name\n",
    "del curated_mof_name\n",
    "del curated_mof_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae88ece",
   "metadata": {},
   "source": [
    "## Taking look at target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfb53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_in_path = os.path.join(PATH, '../input/C3H8-C3H6.csv')\n",
    "#target_in_path = os.path.join(PATH, '../input/C2H6-C2H4.csv')\n",
    "\n",
    "df_target = pd.read_csv(target_in_path)\n",
    "\n",
    "print(f'Target property input DataFrame shape:\\n\\n {df_target.shape}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "print(f'\\nTarget property input data columns:\\n\\n {df_target.columns}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "print(f'\\nTarget property input dataframe head:\\n\\n {df_target.head()}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "del target_in_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2582f91",
   "metadata": {},
   "source": [
    "## Renaming Target property columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb4d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {'MOF_no': 'mof', 'propane_avg(mol/kg)': 'propane_uptake(mol/kg)',\n",
    "              'propylene_avg(mol/kg)': 'propylene_uptake(mol/kg)',\n",
    "              'C3H8/C3H6 Selectivity (1Bar)': 'propane_propylene_selectivity', 'Df': 'PLD',\n",
    "              'AV_Volume_fraction_1.9': 'AVF'}\n",
    "'''\n",
    "\n",
    "rename_dict = {'MOF_no': 'mof', 'ethane_avg(mol/kg)': 'ethane_uptake(mol/kg)',\n",
    "              'ethylene_avg(mol/kg)': 'ethylene_uptake(mol/kg)',\n",
    "              'C2H6/C2H4 Selectivity (1Bar)': 'ethane_ethylene_selectivity', 'Df': 'PLD',\n",
    "              'AV_Volume_fraction_1.9': 'AVF'}\n",
    "\n",
    "'''\n",
    "df_target = df_target.rename(columns=rename_dict)\n",
    "\n",
    "print(f'\\nCurated target columns:\\n\\n {df_target.columns}\\n')\n",
    "print('------------------------------------------------------------')\n",
    "      \n",
    "del rename_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cfe5ff",
   "metadata": {},
   "source": [
    "## Curating Target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_gross1_atomic = df_target\n",
    "\n",
    "# Selecting materials with PLD > 3.8 A\n",
    "\n",
    "df_target_gross1_atomic = df_target_gross1_atomic[(df_target_gross1_atomic['PLD'] > 3.8)]\n",
    "\n",
    "# Selecting material with AVF > 0\n",
    "df_target_gross1_atomic = df_target_gross1_atomic[(df_target_gross1_atomic['AVF'] > 0.0)]\n",
    "\n",
    "target_mof_name = df_target_gross1_atomic['mof'].astype(str)\n",
    "target_mof_name_path = os.path.join(PATH, '../output/target-mof-name.csv')\n",
    "target_mof_name.to_csv(target_mof_name_path, index=False)\n",
    "\n",
    "columns = ['propane_uptake(mol/kg)', 'propane_propylene_selectivity', 'TSN', 'propylene_uptake(mol/kg)']\n",
    "\n",
    "#columns = ['ethane_uptake(mol/kg)', 'ethane_ethylene_selectivity', 'TSN', 'ethylene_uptake(mol/kg)']\n",
    "\n",
    "\n",
    "df_target_gross1_atomic = df_target_gross1_atomic[columns].astype(float)\n",
    "target_mof_prop_path = os.path.join(PATH, '../output/target-mof-prop.csv')\n",
    "\n",
    "print(f'\\nCurated target data:\\n\\n {df_target_gross1_atomic}\\n')\n",
    "print('\\n------------------------------------------------------------\\n')\n",
    "\n",
    "print(f'\\nData type of each column. Note that it should be float\\n\\n {df_target_gross1_atomic.dtypes}\\n')\n",
    "print('\\n------------------------------------------------------------\\n')\n",
    "\n",
    "del df_target\n",
    "del columns\n",
    "del target_mof_name\n",
    "del target_mof_name_path\n",
    "del target_mof_prop_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "profile = ProfileReport(df_join.copy(),title='C3H8-C3H6', html={'style':{'full_width':True}})\n",
    "# profile.to_widgets()\n",
    "#profile.to_notebook_iframe()\n",
    "C3H8_report = os.path.join(PATH, '../output/C3H8-C3H6-report.csv')\n",
    "\n",
    "profile.to_file(\"/home/varad/Pictures/best_model_selection_updated/1_excluding_oms/1_Propane_RACs_excluding.html\")\n",
    "\n",
    "''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_crude = df_descriptor_gross1_atomic\n",
    "Y_crude = df_target_gross1_atomic\n",
    "\n",
    "print(f'\\nShape of X_crude: {X_crude.shape}')\n",
    "print(f'\\nShape of Y_crude: {Y_crude.shape}')\n",
    "\n",
    "del df_descriptor_gross1_atomic\n",
    "del df_target_gross1_atomic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7091485d",
   "metadata": {},
   "source": [
    "Here I implemented some classical ML models from `sklearn`:\n",
    "\n",
    "* Ridge regression\n",
    "* Support vector machine\n",
    "* Linear support vector machine\n",
    "* Random forest\n",
    "* Extra trees\n",
    "* Adaptive boosting\n",
    "* Gradient boosting\n",
    "* k-nearest neighbors\n",
    "* Dummy (if one can't beat this, then our model is wrong.)\n",
    "\n",
    "Note: the Dummy model from `sklearn` act as a good sanity check for our ML studies. If our models does not perform significantly better than the equivalent Dummy models, something is wrong in our model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b544586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb2ae4",
   "metadata": {},
   "source": [
    "In addition, we define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f83be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_model(model_name):\n",
    "    model = model_name()\n",
    "    return model\n",
    "\n",
    "def fit_model(model, X_train, y_train):\n",
    "    ti = time()\n",
    "    model = instantiate_model(model)\n",
    "    model.fit(X_train, y_train)\n",
    "    fit_time = time() - ti\n",
    "    return model, fit_time\n",
    "\n",
    "def evaluate_model(model, X, y_act):\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y_act, y_pred)\n",
    "    mae = mean_absolute_error(y_act, y_pred)\n",
    "    rmse_val = mean_squared_error(y_act, y_pred, squared=False)\n",
    "    return r2, mae, rmse_val\n",
    "\n",
    "def fit_evaluate_model(model, model_name, split, X_train, y_train, X_val, y_act_val):\n",
    "    model, fit_time = fit_model(model, X_train, y_train)\n",
    "    r2_train, mae_train, rmse_train = evaluate_model(model, X_train, y_train)\n",
    "    r2_val, mae_val, rmse_val = evaluate_model(model, X_val, y_act_val)\n",
    "    result_dict = {\n",
    "        'split': split,\n",
    "        'model_name': model_name,\n",
    "        'model_name_pretty': type(model).__name__,\n",
    "        'model_params': model.get_params(),\n",
    "        'fit_time': fit_time,\n",
    "        'r2_train': r2_train,\n",
    "        'mae_train': mae_train,\n",
    "        'rmse_train': rmse_train,\n",
    "        'r2_val': r2_val,\n",
    "        'mae_val': mae_val,\n",
    "        'rmse_val': rmse_val}\n",
    "    return model, result_dict\n",
    "\n",
    "def append_result_df(df, result_dict):\n",
    "    df_result_appended = df.append(result_dict, ignore_index=True)\n",
    "    return df_result_appended\n",
    "\n",
    "def append_model_dict(dic, model_name, model):\n",
    "    dic[model_name] = model\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bacaccd",
   "metadata": {},
   "source": [
    "Build an empty DataFrame to store model results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classics = pd.DataFrame(columns=['split',\n",
    "                                    'model_name',\n",
    "                                    'model_name_pretty',\n",
    "                                    'model_params',\n",
    "                                    'fit_time',\n",
    "                                    'r2_train',\n",
    "                                    'mae_train',\n",
    "                                    'rmse_train',\n",
    "                                    'r2_val',\n",
    "                                    'mae_val',\n",
    "                                    'rmse_val'])\n",
    "df_classics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e5d31",
   "metadata": {},
   "source": [
    "## Define the models\n",
    "\n",
    "Here, I instantiated several classical machine learning models for use.\n",
    "I have not tuned the hyperparameters of the model. And default parametes are used here.\n",
    "Hyper parameters tuning using `Grid search` will be the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fe26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary of model names\n",
    "classic_model_names = OrderedDict({\n",
    "    'dumr': DummyRegressor,\n",
    "    'rr': Ridge,\n",
    "    'abr': AdaBoostRegressor,\n",
    "    'gbr': GradientBoostingRegressor,\n",
    "    'rfr': RandomForestRegressor,\n",
    "    'etr': ExtraTreesRegressor,\n",
    "    'svr': SVR,\n",
    "    'lsvr': LinearSVR,\n",
    "    'knr': KNeighborsRegressor,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036dedf",
   "metadata": {},
   "source": [
    "## Instantiate and fit the models\n",
    "\n",
    "Now, we can fit the ML models.\n",
    "\n",
    "We will loop through each of the models listed above. For each of the models, we will:\n",
    "* instantiate the model (`with default parameters`)\n",
    "* fit the model using the training data\n",
    "* use the fitted model to generate predictions from the validation data\n",
    "* evaluate the performance of the model using the predictions\n",
    "* store the results in a DataFrame for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa0cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_act_mine(Y_act_train, Y_pred_train, Y_act, Y_pred, model, path, scale, prop, cord_list, val):\n",
    "    \n",
    "    # Setting plotting attributes\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "    fontdict_t = {'fontsize': 14, 'weight': 'bold', 'ha': 'center'}\n",
    "    fontdict_x = {'fontsize': 12, 'weight': 'bold', 'ha': 'center'}\n",
    "    fontdict_y = {'fontsize': 12, 'weight': 'bold', 'va': 'baseline', 'ha': 'center'}\n",
    "    \n",
    "    # Plotting\n",
    "    plot = plt.figure(figsize=(6,6))\n",
    "    \n",
    "    #print(model_name)\n",
    "    #print(model.__name__)\n",
    "    #print(model)\n",
    "    \n",
    "    #raise ValueError('Testing going on')\n",
    "    \n",
    "    if val: \n",
    "        #print('Plotting a plot for Train and Validation set')\n",
    "        \n",
    "        # Finding Maximum and minimum for straight line graph\n",
    "        \n",
    "        xy_max = np.max([np.max(Y_act_train), np.max(Y_pred_train)])\n",
    "        xy_min = np.min([np.min(Y_act_train), np.min(Y_pred_train)])\n",
    "        \n",
    "        plt.scatter(Y_act, Y_pred, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "                    label='Validation set')\n",
    "        \n",
    "        plt.scatter(Y_act_train, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "                    label='Train set')\n",
    "        \n",
    "        plt.plot([xy_min, xy_max], [xy_min,xy_max], color='black', linestyle='--')\n",
    "        \n",
    "        #plt.title(f'{type(model).__name__}, r2: {r2_score(act, pred):0.4f}')\n",
    "        plt.title(f'{type(model).__name__} model for \\ntrain and validation set ({scale})',\n",
    "                  fontdict=fontdict_t, color='black')\n",
    "        plt.axis('scaled')\n",
    "        \n",
    "        plt.xlabel(f'GCMC simulated {prop}', fontdict=fontdict_x)\n",
    "        plt.ylabel(f'ML Predicted {prop}', fontdict=fontdict_y)\n",
    "        plt.legend(loc='upper left')\n",
    "        \n",
    "        plt.text(cord_list[0], cord_list[1], str('Train     Validation'), weight='bold', horizontalalignment='left', \n",
    "                 size='medium', color='black', fontsize=10)\n",
    "\n",
    "        plt.text(cord_list[2], cord_list[3], str('$\\mathregular{R^2:}$ ') + '{:.3f}'.format(r2_score(Y_act_train, Y_pred_train))\n",
    "                 + str('   ') + '{:.3f}'.format(r2_score(Y_act, Y_pred)), weight='bold', \n",
    "                 horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "        plt.text(cord_list[4], cord_list[5], str('$\\mathregular{MAE:}$ ') + '{:.3f}'.format(mean_absolute_error(Y_act_train, Y_pred_train)) \n",
    "                 + str('   ') + '{:.3f}'.format(mean_absolute_error(Y_act, Y_pred)), weight='bold', \n",
    "                 horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "        \n",
    "        plt.text(cord_list[6], cord_list[7], str('$\\mathregular{RMSE:}$ ') + '{:.3f}'.format(mean_squared_error(Y_act_train, Y_pred_train, squared = False)) \n",
    "                 + str('   ') + '{:.3f}'.format(mean_squared_error(Y_act, Y_pred, squared = False)), weight='bold', \n",
    "                 horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "        \n",
    "        train_val_path = path + '/' +  '1_train_val' + '/' + str(model.__name__)\n",
    "        \n",
    "        #print (path)\n",
    "        #print(str(model.__name__))\n",
    "        \n",
    "        plt.savefig(train_val_path, dpi=300)\n",
    "        \n",
    "        #raise ValueError('Testing going on val = true!!')\n",
    "        \n",
    "        return plot\n",
    "    \n",
    "    else:\n",
    "        #print('Plotting a plot for Train and Test set')\n",
    "        #print('Note that here the train set is combination of train and validation set')\n",
    "        \n",
    "        # Finding Maximum and minimum for straight line graph\n",
    "        \n",
    "        xy_max = np.max([np.max(Y_act_train), np.max(Y_pred_train)])\n",
    "        xy_min = np.min([np.min(Y_act_train), np.min(Y_pred_train)])\n",
    "        \n",
    "        plt.scatter(Y_act, Y_pred, s=30, c='green', edgecolor='black', linewidth=1, alpha=0.75, label='Test set')\n",
    "        \n",
    "        plt.scatter(Y_act_train, Y_pred_train, s=30, c='red', edgecolor='black', linewidth=1, alpha=0.75, \n",
    "                    label='Train set')\n",
    "        \n",
    "        plt.plot([xy_min, xy_max], [xy_min,xy_max], color='black', linestyle='--')\n",
    "        \n",
    "        #plt.title(f'{type(model).__name__}, r2: {r2_score(act, pred):0.4f}')\n",
    "        plt.title(f'{type(model).__name__} model for \\ntrain and test set ({scale})',\n",
    "                  fontdict=fontdict_t, color='black')\n",
    "        plt.axis('scaled')\n",
    "        \n",
    "        plt.xlabel(f'GCMC simulated {prop}', fontdict=fontdict_x)\n",
    "        plt.ylabel(f'ML Predicted {prop}', fontdict=fontdict_y)\n",
    "        plt.legend(loc='upper left')\n",
    "        \n",
    "        plt.text(cord_list[0], cord_list[1], str('Train     Test'), weight='bold', horizontalalignment='left', \n",
    "                 size='medium', color='black', fontsize=10)\n",
    "\n",
    "        plt.text(cord_list[2], cord_list[3], str('$\\mathregular{R^2:}$ ') + '{:.3f}'.format(r2_score(Y_act_train, Y_pred_train))\n",
    "                 + str('   ') + '{:.3f}'.format(r2_score(Y_act, Y_pred)), weight='bold', \n",
    "                 horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "\n",
    "        plt.text(cord_list[4], cord_list[5], str('$\\mathregular{MAE:}$ ') + '{:.3f}'.format(mean_absolute_error(Y_act_train, Y_pred_train)) \n",
    "                 + str('   ') + '{:.3f}'.format(mean_absolute_error(Y_act, Y_pred)), weight='bold', \n",
    "                 horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "        \n",
    "        plt.text(cord_list[6], cord_list[7], str('$\\mathregular{RMSE:}$ ') + '{:.3f}'.format(mean_squared_error(Y_act_train, Y_pred_train, squared = False)) \n",
    "                 + str('   ') + '{:.3f}'.format(mean_squared_error(Y_act, Y_pred, squared = False)), weight='bold', \n",
    "                 horizontalalignment='left', size='medium', color='black', fontsize=10)\n",
    "        \n",
    "        train_test_path = path + '/' + '2_train_test' + '/' + str(model.__name__)\n",
    "        \n",
    "        #print (path)\n",
    "        #print(str(type(model).__name__))\n",
    "        \n",
    "        plt.savefig(train_test_path, dpi=300)\n",
    "        \n",
    "        #raise ValueError('Testing going on val = true!!')\n",
    "        \n",
    "        return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4ca01",
   "metadata": {},
   "source": [
    "# Creating validation set and using the same validation set for all the random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_val_crude, Y, Y_val_crude = train_test_split(X_crude, Y_crude, test_size=0.32, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a dictionary to store the model objects\n",
    "classic_models = OrderedDict()\n",
    "\n",
    "# Keep track of elapsed time\n",
    "ti = time()\n",
    "\n",
    "# base path\n",
    "base_path = os.path.join(PATH, '../output/best_model/')\n",
    "\n",
    "# Mixture status\n",
    "mixture_status = \"1_Propane/\"\n",
    "#mixture_status = \"2_Ethane/\"\n",
    "\n",
    "# Which Property is used as target variable\n",
    "property_status = \"1_selectivity/\"\n",
    "#property_status = \"2_uptake_paraffin/\"\n",
    "#property_status = \"3_uptake_olefin/\"\n",
    "#property_status = \"4_TSN/\"\n",
    "\n",
    "# Whether atomic features are used or RACs are used\n",
    "feature_status = \"1_Atomic/\"\n",
    "#feature_status = \"2_RACs/\"\n",
    "\n",
    "# Combined path\n",
    "comb_path = base_path + feature_status + property_status + mixture_status\n",
    "\n",
    "# A dataframe to get the average r2 for all the splits of all the models\n",
    "df_average = pd.DataFrame(columns=['model_name',\n",
    "                                   'model_name_pretty',\n",
    "                                   '<r2_train>',\n",
    "                                   '<MAE_train>',\n",
    "                                   '<RMSE_train>',\n",
    "                                   '<r2_val>',\n",
    "                                   '<MAE_val>',\n",
    "                                   '<RMSE_val>',\n",
    "                                   '<r2_new_train>',\n",
    "                                   '<MAE_new_train>',\n",
    "                                   '<RMSE_new_train>',\n",
    "                                   '<r2_test>',\n",
    "                                   '<MAE_test>',\n",
    "                                   '<RMSE_test>'])\n",
    "\n",
    "\n",
    "# Loop through each model type, fit and predict, and evaluate and store results\n",
    "for model_name_temp, model_temp in classic_model_names.items():\n",
    "    #print(model_name)\n",
    "    #print(model.__name__)\n",
    "    #print(model)\n",
    "    #splits = range(10)\n",
    "    \n",
    "    # Model is selected\n",
    "    \n",
    "    splits = [41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
    "    \n",
    "    df_classics_val = pd.DataFrame(columns=['split',\n",
    "                                    'model_name',\n",
    "                                    'model_name_pretty',\n",
    "                                    'model_params',\n",
    "                                    'fit_time',\n",
    "                                    'r2_train',\n",
    "                                    'mae_train',\n",
    "                                    'rmse_train',\n",
    "                                    'r2_val',\n",
    "                                    'mae_val',\n",
    "                                    'rmse_val'])\n",
    "    \n",
    "    df_classics_test = df_classics_val\n",
    "    \n",
    "    for split in splits:\n",
    "        \n",
    "        # Random splits for the model selected\n",
    "        \n",
    "        model_name = model_name_temp\n",
    "        model      = model_temp\n",
    "        \n",
    "        #print('----------------')\n",
    "        #print(model_name)\n",
    "        #print(model.__name__)\n",
    "        #print(model)\n",
    "        #print('----------------')\n",
    "        \n",
    "        print(f'Fitting and evaluating model {model_name}: {model.__name__} for random seed of {split}')\n",
    "        #print(f'Fitting and evaluating model {model_name} for random seed of {split}')\n",
    "        \n",
    "        # Creating the test train split\n",
    "        \n",
    "        X_train_crude, X_test_crude, Y_train_crude, Y_test_crude = train_test_split(X, Y, test_size=0.294, random_state=split)\n",
    "\n",
    "        #-----------------------------------------------------------------------------------------------------#\n",
    "        ## For Learning curve\n",
    "        \n",
    "        #print(f'\\n X_train is :\\n\\n {X_train_crude}\\n')\n",
    "        #print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "        \n",
    "        #print(f'\\n X_val is :\\n\\n {X_val_crude}\\n')\n",
    "        #print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "        \n",
    "        #print(f'\\n X_test is :\\n\\n {X_test_crude}\\n')\n",
    "        #print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "        #raise ValueError('Testing going on!!')\n",
    "        \n",
    "#************************************************************************************************************#        \n",
    "        # Scaling the data\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        X_train_scaled = scaler.fit_transform(X_train_crude)\n",
    "        X_val_scaled   = scaler.transform(X_val_crude)\n",
    "        X_test_scaled  = scaler.transform(X_test_crude)\n",
    "        \n",
    "        # Normalizing the unscaled data\n",
    "        norm = MinMaxScaler().fit(X_train_crude)\n",
    "\n",
    "        X_train_norm  = norm.transform(X_train_crude)\n",
    "        X_val_norm    = norm.transform(X_val_crude)\n",
    "        X_test_norm   = norm.transform(X_test_crude)\n",
    "        \n",
    "        # Normalizing the scaled data\n",
    "        norm_scaled         = MinMaxScaler().fit(X_train_scaled)\n",
    "\n",
    "        X_train_scaled_norm = norm_scaled.transform(X_train_scaled)\n",
    "        X_val_scaled_norm   = norm_scaled.transform(X_val_scaled)\n",
    "        X_test_scaled_norm  = norm_scaled.transform(X_test_scaled)\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "        \n",
    "#***********************************************************************************************************#        \n",
    "        \n",
    "        ## Uncomment when model has to be trained on crude data\n",
    "        #X_train = X_train_crude\n",
    "        #X_val   = X_val_crude\n",
    "        #X_test  = X_test_crude\n",
    "\n",
    "        ## Uncomment when model has to be trained on scaled data\n",
    "\n",
    "        #X_train = X_train_scaled\n",
    "        #X_val   = X_val_scaled\n",
    "        #X_test  = X_test_scaled\n",
    "\n",
    "        ## Uncomment when model has to be trained on normalised data\n",
    "        #X_train = X_train_norm\n",
    "        #X_val   = X_val_norm\n",
    "        #X_test  = X_test_norm\n",
    "\n",
    "        ## Uncomment when model has to be trained on scaled_normalised  data\n",
    "        X_train  = X_train_scaled_norm\n",
    "        X_val    = X_val_scaled_norm\n",
    "        X_test   = X_test_scaled_norm\n",
    "        \n",
    "#***********************************************************************************************************# \n",
    "\n",
    "#***********************************************************************************************************#\n",
    "        \n",
    "        # Target Y is neigther scaled nor normalized\n",
    "    \n",
    "        # If index is 0 then, propane / ethane uptake (mol/kg)  \n",
    "        # If index is 1 then, selectivity\n",
    "        # If index is 2 then, TSN\n",
    "        # If index is 3 then, propylene / ethylene uptake (mol/kg)\n",
    "\n",
    "        i = 1\n",
    "        \n",
    "        Y_target_train = Y_train_crude.iloc[:,i]\n",
    "        Y_target_test  = Y_test_crude.iloc[:,i]\n",
    "        Y_target_val   = Y_val_crude.iloc[:,i]\n",
    "        \n",
    "        # Note feature status = atomic or RAC does not matter as cord_list does not change\n",
    "               \n",
    "        #--------------------------------------------------------------------------------------------------#\n",
    "        # Propane + {property} + {atomic / RAC (doesn't matter)} + including + scaled + normalized #\n",
    "        \n",
    "        elif (i == 1 and mixture_status == \"1_Propane/\" and property_status == \"1_selectivity/\"):\n",
    "            print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"$S_{C_{3}H_{8}/C_{3}H_{6}}$\"\n",
    "            cord_list = [1.75, 1.20, 1.65, 1.13, 1.59, 1.08, 1.56, 1.03]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "\n",
    "        elif (i == 0 and mixture_status == \"1_Propane/\" and property_status == \"2_uptake_paraffin/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"$N_{C_{3}H_{8}}$\"\n",
    "            cord_list = [1.0, 0.3, 0.85, 0.2, 0.78, 0.13, 0.74, 0.06]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "        \n",
    "        elif (i == 3 and mixture_status == \"1_Propane/\" and property_status == \"3_uptake_olefin/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"$N_{C_{3}H_{6}}$\"\n",
    "            cord_list = [4.0, 1.3, 3.4, 1.0, 3.1, 0.7, 2.9, 0.4]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "            \n",
    "        elif (i == 2 and mixture_status == \"1_Propane/\" and property_status == \"4_TSN/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"TSN\"\n",
    "            cord_list = [0.13, 0.04, 0.110, 0.025, 0.099, 0.013, 0.094, 0.001]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "            \n",
    "        #--------------------------------------------------------------------------------------------------#\n",
    "        \n",
    "        #--------------------------------------------------------------------------------------------------#\n",
    "        # Ethane + {property} + {atomic / RAC (doesn't matter)} + including + scaled + normalized #\n",
    "        \n",
    "        elif (i == 1 and mixture_status == \"2_Ethane/\" and property_status == \"1_selectivity/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"$S_{C_{2}H_{6}/C_{2}H_{4}}$\"\n",
    "            cord_list = [2.25, 1.2, 2.06, 1.05, 1.95, 0.92, 1.89, 0.79]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "\n",
    "        elif (i == 0 and mixture_status == \"2_Ethane/\" and property_status == \"2_uptake_paraffin/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"$N_{C_{2}H_{6}}$\"\n",
    "            cord_list = [0.40, 0.1, 0.35, 0.06, 0.32, 0.03, 0.305, 0.00]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "        \n",
    "        elif (i == 3 and mixture_status == \"2_Ethane/\" and property_status == \"3_uptake_olefin/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"$N_{C_{2}H_{4}}$\"\n",
    "            cord_list = [1.75, 0.5, 1.5, 0.3, 1.38, 0.15, 1.32, 0.0]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "            \n",
    "        elif (i == 2 and mixture_status == \"2_Ethane/\" and property_status == \"4_TSN/\"):\n",
    "            #print(\"This should be running\")\n",
    "            scale  = \"Scaled + Normalized\"\n",
    "            prop   = \"TSN\"\n",
    "            cord_list = [0.1, 0.040, 0.085, 0.025, 0.077, 0.013, 0.073, 0.001]\n",
    "            #raise ValueError('Testing going on!!')\n",
    "        else :\n",
    "            raise ValueError('Combinations are wrong')\n",
    "        \n",
    "        #--------------------------------------------------------------------------------------------------#\n",
    "        \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "        #print('************')\n",
    "        #print(model_name)\n",
    "        #print(model.__name__)\n",
    "        #print(model)\n",
    "        #print('************')\n",
    "        \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "# Evaluating model performance on train and same validation set for different models\n",
    "\n",
    "        model_val, result_dict_val = fit_evaluate_model(model, model_name, split, X_train, Y_target_train, X_val, Y_target_val)\n",
    "        \n",
    "        df_classics_val = append_result_df(df_classics_val, result_dict_val)\n",
    "        \n",
    "        Y_act_train  = Y_target_train\n",
    "        Y_pred_train = model_val.predict(X_train)\n",
    "        \n",
    "        Y_act_val  = Y_target_val\n",
    "        Y_pred_val = model_val.predict(X_val)\n",
    "        \n",
    "        model_performance_path_val = comb_path  + str(model_name_temp) + '/' + 'split_' + str(split)\n",
    "        \n",
    "        plot = plot_pred_act_mine(Y_act_train, Y_pred_train, Y_act_val, Y_pred_val, model, model_performance_path_val, scale, prop, cord_list, val=True)\n",
    "        #raise ValueError('Testing going on!!')\n",
    "        \n",
    "        del model_val\n",
    "        del Y_act_train, Y_pred_train, Y_act_val, Y_pred_val\n",
    "        del model_performance_path_val\n",
    "        \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "\n",
    "# Evaluating model performance on new train set and test set for different models\n",
    "# Actually this process should be done on the best model selected in previous step\n",
    "# However, I am doing predictions on test set for all the models. This is done only for analysis purposes.\n",
    "\n",
    "        X_train_new = np.concatenate((X_train, X_val), axis=0)\n",
    "        Y_train_new = pd.concat((Y_target_train, Y_target_val), axis=0)\n",
    "        \n",
    "        model_test, result_dict_test = fit_evaluate_model(model, model_name, split, X_train_new, Y_train_new, X_test, Y_target_test)\n",
    "        \n",
    "        df_classics_test = append_result_df(df_classics_test, result_dict_test)\n",
    "        \n",
    "        Y_act_train  = Y_train_new\n",
    "        Y_pred_train = model_test.predict(X_train_new)\n",
    "        \n",
    "        Y_act_test  = Y_target_test\n",
    "        Y_pred_test = model_test.predict(X_test)\n",
    "        \n",
    "        model_performance_path_test = comb_path  + str(model_name_temp) + '/' + 'split_' + str(split)\n",
    "        \n",
    "        plot = plot_pred_act_mine(Y_act_train, Y_pred_train, Y_act_test, Y_pred_test, model, model_performance_path_test, scale, prop, cord_list, val=False)\n",
    "        \n",
    "        del X_train_new, Y_train_new\n",
    "        del model_test, Y_act_train, Y_pred_train, Y_act_test, Y_pred_test\n",
    "        del model_performance_path_test\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "        \n",
    "        del model_name, model\n",
    "        #del X, X_test_crude, Y, Y_test_crude\n",
    "        del X_test_crude, Y_test_crude\n",
    "        #del X_train_crude, X_val_crude, Y_train_crude, Y_val_crude\n",
    "        del X_train_crude, Y_train_crude\n",
    "        del scaler, X_train_scaled, X_val_scaled, X_test_scaled, \n",
    "        del norm, X_train_norm, X_val_norm, X_test_norm\n",
    "        del norm_scaled, X_train_scaled_norm, X_val_scaled_norm, X_test_scaled_norm\n",
    "        del X_train, X_val, X_test\n",
    "        del Y_target_train, Y_target_test, Y_target_val\n",
    "        \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "\n",
    "    #raise ValueError('Testing going on!!')\n",
    "    \n",
    "    df_classics_val['split'] = df_classics_val['split'].astype(int)\n",
    "    \n",
    "    print(f'\\n df_classics_val for model {model_temp.__name__} is :\\n\\n {df_classics_val}\\n')\n",
    "    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    split_stat_path_val = comb_path  + str(model_name_temp) + '/' + str(model_name_temp) + '_' +'train_val' + '.csv'\n",
    "    \n",
    "    #print(split_stat_path_val)\n",
    "    \n",
    "    df_classics_val.to_csv(split_stat_path_val, index=False)\n",
    "    \n",
    "    #raise ValueError('Testing going on!!')\n",
    "    \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "    \n",
    "    # Print the average R2, MAE and RMSE for all the splits of a particular model for train set of train_val\n",
    "    \n",
    "    avg_r2_train   = df_classics_val['r2_train'].mean()\n",
    "    avg_mae_train  = df_classics_val['mae_train'].mean()\n",
    "    avg_rmse_train = df_classics_val['rmse_train'].mean()\n",
    "    \n",
    "    print(f'Average train r2 for train-val set for model {model_temp.__name__} is       : {avg_r2_train:0.4f}')\n",
    "    print(f'Average train MAE for train-val set for for model {model_temp.__name__} is  : {avg_mae_train:0.4f}')\n",
    "    print(f'Average train RMSE for train-val set for for model {model_temp.__name__} is : {avg_rmse_train:0.4f}')\n",
    "    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    # Print the average R2, MAE and RMSE for all the splits of a particular model for validation set of train_val\n",
    "    \n",
    "    avg_r2_val   = df_classics_val['r2_val'].mean()\n",
    "    avg_mae_val  = df_classics_val['mae_val'].mean()\n",
    "    avg_rmse_val = df_classics_val['rmse_val'].mean()\n",
    "\n",
    "    print(f'Average validation r2 for train-val set for model {model_temp.__name__} is       : {avg_r2_val:0.4f}')\n",
    "    print(f'Average validation MAE for train-val set for for model {model_temp.__name__} is  : {avg_mae_val:0.4f}')\n",
    "    print(f'Average validation RMSE for train-val set for for model {model_temp.__name__} is : {avg_rmse_val:0.4f}')\n",
    "    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "\n",
    "    \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "    \n",
    "# Note here train = new_train and val = test\n",
    "\n",
    "    df_classics_test['split'] = df_classics_test['split'].astype(int)\n",
    "    \n",
    "    #print(f'\\n df_classics_test for model {model_temp.__name__} is :\\n\\n {df_classics_test}\\n')\n",
    "    #print('\\n------------------------------------------------------------\\n')\n",
    "    \n",
    "    split_stat_path_test = comb_path  + str(model_name_temp) + '/' + str(model_name_temp) + '_' + 'train_test' + '.csv'\n",
    "    \n",
    "    #print(split_stat_path_test)\n",
    "    \n",
    "    df_classics_test.to_csv(split_stat_path_test, index=False)\n",
    "    \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "    \n",
    "    # Print the average R2, MAE and RMSE for all the splits of a particular model for new train_set of for train_val\n",
    "    \n",
    "    avg_r2_new_train   = df_classics_test['r2_train'].mean() # Note here train = new_train and val = test\n",
    "    avg_mae_new_train  = df_classics_test['mae_train'].mean() # Note here train = new_train and val = test \n",
    "    avg_rmse_new_train = df_classics_test['rmse_train'].mean() # Note here train = new_train and val = test \n",
    "\n",
    "    print(f'Average new_train r2 for train-test set for model {model_temp.__name__} is   : {avg_r2_new_train:0.4f}')\n",
    "    print(f'Average new_train MAE for train-test set for model {model_temp.__name__} is  : {avg_mae_new_train:0.4f}')\n",
    "    print(f'Average new_train RMSE for train-test set for model {model_temp.__name__} is : {avg_rmse_new_train:0.4f}')\n",
    "    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "    \n",
    "    # Print the average R2, MAE and RMSE for all the splits of a particular model for test set for train_val\n",
    "    \n",
    "    avg_r2_test   = df_classics_test['r2_val'].mean() # Note here train = new_train and val = test\n",
    "    avg_mae_test  = df_classics_test['mae_val'].mean() # Note here train = new_train and val = test \n",
    "    avg_rmse_test = df_classics_test['rmse_val'].mean() # Note here train = new_train and val = test \n",
    "\n",
    "    print(f'Average validation r2 for train-test set for model {model_temp.__name__} is   : {avg_r2_test:0.4f}')\n",
    "    print(f'Average validation MAE for train-test set for model {model_temp.__name__} is  : {avg_mae_test:0.4f}')\n",
    "    print(f'Average validation RMSE for train-test set for model {model_temp.__name__} is : {avg_rmse_test:0.4f}')\n",
    "    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "    # Here we are calculating average value of R2, MAE, RMSe for all the 10 splits of a particular order\n",
    "    \n",
    "    average_dict = {\n",
    "        'model_name': model_temp.__name__,\n",
    "        'model_name_pretty': model_name_temp,\n",
    "        '<r2_train>': avg_r2_train,\n",
    "        '<MAE_train>': avg_mae_train,\n",
    "        '<RMSE_train>': avg_rmse_train,\n",
    "        '<r2_val>': avg_r2_val,\n",
    "        '<MAE_val>': avg_mae_val,\n",
    "        '<RMSE_val>': avg_rmse_val,\n",
    "        '<r2_new_train>': avg_r2_new_train,\n",
    "        '<MAE_new_train>': avg_mae_new_train,\n",
    "        '<RMSE_new_train>': avg_rmse_new_train,\n",
    "        '<r2_test>': avg_r2_test,\n",
    "        '<MAE_test>': avg_mae_test,\n",
    "        '<RMSE_test>': avg_rmse_test}\n",
    "        \n",
    "    df_average = append_result_df(df_average, average_dict)\n",
    "    \n",
    "    print(f'\\n df_average is :\\n\\n {df_average}\\n')\n",
    "    print('\\n----------------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    del avg_r2_val, avg_mae_val, avg_rmse_val\n",
    "    del avg_r2_test, avg_mae_test, avg_rmse_test\n",
    "    \n",
    "    #raise ValueError('Testing going on!!')\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "\n",
    "#***********************************************************************************************************#\n",
    "    \n",
    "    #We then plot the train and validation $r^2$ scores for each of the 10 models.\n",
    "\n",
    "    #Note the high variability in the r2_val score. In contrast, the variability in the r2_train score is comparatively lower.\n",
    "    \n",
    "    df_classics_val.plot('split', ['r2_train', 'r2_val'], kind='bar')\n",
    "    plt.title(f'Performance of {model_temp.__name__}\\nwith {len(splits)} different data splits')\n",
    "    plt.ylim((0.0, 1.0))\n",
    "    plt.ylabel('$r^2$')\n",
    "    plt.xlabel('Split #')\n",
    "    plt.legend(loc='lower right', framealpha=0.9)\n",
    "    #plt.show()\n",
    "    histo_R2_path = comb_path  + str(model_name_temp) + '/' + str(model_name_temp) + '_' + 'R2_histo.png' \n",
    "    plt.savefig(histo_R2_path, dpi=300)\n",
    "    del histo_R2_path\n",
    "    \n",
    "    df_classics_val.plot('split', ['mae_train', 'mae_val'], kind='bar')\n",
    "    plt.title(f'Performance of {model_temp.__name__}\\nwith {len(splits)} different data splits')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Split #')\n",
    "    plt.legend(loc='lower right', framealpha=0.9)\n",
    "    #plt.show()\n",
    "    histo_MAE_path = comb_path  + str(model_name_temp) + '/' + str(model_name_temp) + '_' + 'MAE_histo.png' \n",
    "    plt.savefig(histo_MAE_path, dpi=300)\n",
    "    del histo_MAE_path \n",
    "    \n",
    "    df_classics_val.plot('split', ['rmse_train', 'rmse_val'], kind='bar')\n",
    "    plt.title(f'Performance of {model_temp.__name__}\\nwith {len(splits)} different data splits')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xlabel('Split #')\n",
    "    plt.legend(loc='lower right', framealpha=0.9)\n",
    "    #plt.show()\n",
    "    histo_RMSE_path = comb_path  + str(model_name_temp) + '/' + str(model_name_temp) + '_' + 'RMSE_histo.png' \n",
    "    plt.savefig(histo_RMSE_path, dpi=300)\n",
    "    del histo_RMSE_path\n",
    "    \n",
    "    #a = df_classics_val\n",
    "    \n",
    "    del df_classics_val, df_classics_test\n",
    "\n",
    "    #raise ValueError('Testing going on!!')\n",
    "    \n",
    "#***********************************************************************************************************#\n",
    "    \n",
    "#***********************************************************************************************************#\n",
    "\n",
    "# Sort in order of decreasing validation r2 score\n",
    "\n",
    "print(f'\\n df_average before sorting is :\\n\\n {df_average}\\n')\n",
    "print('\\n--------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "df_average = df_average.sort_values('<r2_val>', ascending=False, ignore_index=True)\n",
    "\n",
    "print(f'\\n df_average after sorting is :\\n\\n {df_average}\\n')\n",
    "print('\\n--------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "# Saving the sorted df_average\n",
    "\n",
    "df_average_path = comb_path + 'sorted_average_scores.csv'\n",
    "\n",
    "df_average.to_csv(df_average_path, index=False)\n",
    "\n",
    "del df_average_path\n",
    "\n",
    "# Find the best-performing model that we have tested\n",
    "best_row = df_average.iloc[0, :].copy()\n",
    "\n",
    "# Get the model type and model parameters\n",
    "best_model    = best_row['model_name']\n",
    "best_avg_r2   = best_row['<r2_val>']\n",
    "#best_avg_mae  = best_row['<MAE_val>']\n",
    "#best_avg_rmse = best_row['<RMSE_val>']\n",
    "\n",
    "\n",
    "print(f'\\n The best model is {best_model} with an average r2 of {best_avg_r2}\\n')\n",
    "print('\\n--------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "#model_params = best_row['model_params']\n",
    "\n",
    "# Instantiate the model again using the parameters\n",
    "#model = classic_model_names[model_name](**model_params)\n",
    "#print(model)\n",
    "\n",
    "print('\\n--------------------------------------------------------------------------------------------------\\n')\n",
    "print('------------------------------------------------------------')\n",
    "print(style.BOLD + '\\n Options for this run are :' + style.END)\n",
    "print(f'\\nFeature status : {feature_status}')\n",
    "print(f'\\nProperty_status: {property_status}')\n",
    "print(f'\\nMixture_status : {mixture_status}')\n",
    "print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError('Testing going on!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "295.844px",
    "left": "612.667px",
    "right": "20px",
    "top": "84px",
    "width": "544px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
